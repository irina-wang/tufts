{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name and ID\n",
    "\n",
    "Mengqi Irina Wang\n",
    "1278675/mwang17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW05 Code\n",
    "\n",
    "\n",
    "You will complete the following notebook, as described in the PDF for Homework 05 (included in the download with the starter code).  You will submit:\n",
    "1. This notebook file, along with your COLLABORATORS.txt file and the two tree images (PDFs generated using `graphviz` within the code), to the Gradescope link for code.\n",
    "2. A PDF of this notebook and all of its output, once it is completed, to the Gradescope link for the PDF.\n",
    "\n",
    "\n",
    "Please report any questions to the [class Piazza page](https://piazza.com/tufts/spring2021/comp135).\n",
    "\n",
    "### Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.tree\n",
    "import graphviz\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(print_changed_only=False)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "You should start by computing the two heuristic values for the toy data described in the assignment handout. You should then load the two versions of the abalone data, compute the two heuristic values on features (for the simplified data), and then build decision trees for each set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Compute both heuristics for toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = np.array(\n",
    "   [[1, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1]])\n",
    "toy_features = np.array(['A', 'B'])\n",
    "\n",
    "toy_X = toy[:, :2]\n",
    "toy_y = toy[:, 2]\n",
    "\n",
    "num_feat = len(toy_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Compute the counting-based heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acc = np.zeros(num_feat)\n",
    "total_data = len(toy_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_feat):\n",
    "    toy_x = toy_X[:,i].reshape(-1, 1)\n",
    "    model = sklearn.tree.DecisionTreeClassifier()\n",
    "    model.fit(toy_x, toy_y)\n",
    "    pred = model.predict(toy_x)\n",
    "    feature_acc[i] = len(np.where(pred == toy_y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 6 / 8\n",
      "B: 6 / 8\n"
     ]
    }
   ],
   "source": [
    "seq = np.argsort(-feature_acc)\n",
    "for i in seq:\n",
    "    print('%s: %i / %i' % (toy_features[i], feature_acc[i], total_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Compute the information-theoretic heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(p,n):\n",
    "    prob_p = p / (p+n)\n",
    "    prob_n = n / (p+n)\n",
    "\n",
    "    class1 = prob_p*(np.log2(prob_p)) if (prob_p != 0) else 0      \n",
    "    class2 = prob_n*(np.log2(prob_n)) if (prob_n != 0) else 0\n",
    "    h = - (class1 + class2)\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pos = len(np.nonzero(toy_y)[0])\n",
    "pre_neg = total_data - len(np.nonzero(toy_y)[0])\n",
    "\n",
    "pre_entro = calc_entropy(pre_pos,pre_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_entro = np.zeros(num_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remainder_entropy():\n",
    "    for i in range(num_feat):\n",
    "        post_true = np.where(toy_X[:, i] == 1)\n",
    "        post_false = np.where(toy_X[:, i] == 0)\n",
    "        \n",
    "        post_true_pos = len(np.where(toy_y[post_true] == 1)[0])\n",
    "        post_true_neg = len(np.where(toy_y[post_true] == 0)[0])\n",
    "        post_false_pos = len(np.where(toy_y[post_false] == 1)[0])\n",
    "        post_false_neg = len(np.where(toy_y[post_false] == 0)[0])\n",
    "        \n",
    "        prob_post_true =  (post_true_pos + post_true_neg) / total_data\n",
    "        prob_post_false =  (post_false_pos + post_false_neg) / total_data\n",
    "        \n",
    "        post_entro[i] = prob_post_true * calc_entropy(post_true_pos,post_true_neg) + prob_post_false * calc_entropy(post_false_pos,post_false_neg)\n",
    "        \n",
    "\n",
    "remainder_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain = np.zeros(num_feat)\n",
    "def calc_info_gain():\n",
    "    for i in range(num_feat):\n",
    "        info_gain[i] = pre_entro - post_entro[i]\n",
    "        \n",
    "calc_info_gain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 0.311\n",
      "B: 0.189\n"
     ]
    }
   ],
   "source": [
    "seq = np.argsort(-info_gain)\n",
    "for i in seq:\n",
    "    print('%s: %.3f' % (toy_features[i], info_gain[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Discussion of results.\n",
    "\n",
    "The result shows that both features perform equally good in terms of correctness(counting-based heuristic), thus, using solely the counting-based heuristic, we won't be able to decide which feature is better. However, the infomation gain method successfully separate and order the two features --  feature A is better at reducing entropy, as showed through its high information gain value. Thus, if we built a tree using each of these two heuristics, knowing they will be equally good in terms of the counting-based heuristic, feature A will give us more information on the toy dataset than feature B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Compute both heuristics for simplified abalone data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_x_train = np.loadtxt('data_abalone/small_binary_x_train.csv', skiprows=1, delimiter=',')\n",
    "simple_x_test = np.loadtxt('data_abalone/small_binary_x_test.csv', skiprows=1, delimiter=',')\n",
    "\n",
    "three_class_y_train = np.loadtxt('data_abalone/3class_y_train.csv', skiprows=1, delimiter=',')\n",
    "three_class_y_test = np.loadtxt('data_abalone/3class_y_test.csv', skiprows=1, delimiter=',')\n",
    "\n",
    "assert simple_x_train.shape[0] == three_class_y_train.shape[0]\n",
    "assert simple_x_test.shape[0] == three_class_y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_features = np.array(['is_male','length_mm','diam_mm','height_mm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_classes = np.array([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_classes_labels = np.array([\"small\",\"medium\",\"large\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = len(simple_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(simple_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Compute the counting-based heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_correct = np.zeros(num_feat)\n",
    "total_data = len(simple_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_feat):\n",
    "    selected_simple_x_train = simple_x_train[:, i].reshape(-1,1)\n",
    "    model = sklearn.tree.DecisionTreeClassifier()\n",
    "    model.fit(selected_simple_x_train, three_class_y_train)\n",
    "    pred = model.predict(selected_simple_x_train)\n",
    "    feature_correct[i] = len(np.where(pred == three_class_y_train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height_mm: 2316 / 3176\n",
      "diam_mm: 2266 / 3176\n",
      "length_mm: 2230 / 3176\n",
      "is_male: 1864 / 3176\n"
     ]
    }
   ],
   "source": [
    "seq = np.argsort(-feature_correct)\n",
    "for i in seq:\n",
    "    print('%s: %i / %i' % (simple_features[i], feature_correct[i], total_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Compute the information-theoretic heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(data, num_class):\n",
    "    total_sum = np.sum(data)\n",
    "    class_entro = np.zeros(num_class)\n",
    "    \n",
    "    for i in range(num_class):\n",
    "        class_entro[i] = (data[i] / total_sum) * (np.log2(data[i] / total_sum)) if (data[i] != 0) else 0\n",
    "    \n",
    "    h = - (np.sum(class_entro))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pre -class data \n",
    "pre_class = np.zeros(num_class)\n",
    "\n",
    "for i in simple_classes:\n",
    "    pre_class[i] = len(np.where(three_class_y_train == i)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_entropy = calc_entropy(pre_class, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_remainder_entropy(X, y, E_val, num_class):\n",
    "    E = np.zeros([len(E_val), num_class])\n",
    "    p = np.zeros(len(E_val))\n",
    "    post_entropy = np.zeros(len(E_val))\n",
    "    for k in E_val:\n",
    "        E_k = np.where(X == k)\n",
    "        for i in range(num_class):\n",
    "            E[k][i] = len(np.where(y[E_k] == i)[0])\n",
    "            \n",
    "        post_entropy[k] = calc_entropy(E[k], num_class)\n",
    "        p[k] = len(E_k[0]) / len(X)\n",
    "        \n",
    "        info_gain = pre_entropy - (p[0] * post_entropy[0] + p[1] * post_entropy[1])\n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.array([0,1])\n",
    "simple_info_gain = np.zeros(num_feat)\n",
    "for k in range(num_feat):\n",
    "    simple_info_gain[k] = single_remainder_entropy(simple_x_train[:,k], three_class_y_train, e, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height_mm: 0.173\n",
      "diam_mm: 0.150\n",
      "length_mm: 0.135\n",
      "is_male: 0.025\n"
     ]
    }
   ],
   "source": [
    "seq = np.argsort(-simple_info_gain)\n",
    "for i in seq:\n",
    "    print('%s: %.3f' % (simple_features[i], simple_info_gain[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Generate decision trees for full- and restricted-feature data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.loadtxt('data_abalone/x_train.csv', skiprows=1, delimiter=',')\n",
    "x_test = np.loadtxt('data_abalone/x_test.csv', skiprows=1, delimiter=',')\n",
    "\n",
    "y_train = np.loadtxt('data_abalone/y_train.csv', skiprows=1, delimiter=',')\n",
    "y_test = np.loadtxt('data_abalone/y_test.csv', skiprows=1, delimiter=',')\n",
    "\n",
    "assert x_train.shape[0] == y_train.shape[0]\n",
    "assert x_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Print accuracy values and generate tree images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_clf = sklearn.tree.DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for simple train data is 0.733\n",
      "accuracy score for simple test data is 0.722\n"
     ]
    }
   ],
   "source": [
    "# simple train \n",
    "simple_clf.fit(simple_x_train, three_class_y_train)\n",
    "acc_simple_train = simple_clf.score(simple_x_train, three_class_y_train)\n",
    "acc_simple_test = simple_clf.score(simple_x_test, three_class_y_test)\n",
    "\n",
    "print('accuracy score for simple train data is %.3f' % acc_simple_train)\n",
    "print('accuracy score for simple test data is %.3f' % acc_simple_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simple tree graph.pdf'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_tree = sklearn.tree.export_graphviz(simple_clf, class_names =simple_classes_labels,  filled=True)\n",
    "graph = graphviz.Source(simple_tree) \n",
    "graph.render(\"simple tree graph\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_clf = sklearn.tree.DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for general train data is 1.000\n",
      "accuracy score for general test data is 0.178\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "general_clf.fit(x_train, y_train)\n",
    "acc_train = general_clf.score(x_train, y_train)\n",
    "acc_test = general_clf.score(x_test, y_test)\n",
    "\n",
    "print('accuracy score for general train data is %.3f' % acc_train)\n",
    "print('accuracy score for general test data is %.3f' % acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tree graph.pdf'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = sklearn.tree.export_graphviz(general_clf,  filled=True)\n",
    "graph = graphviz.Source(tree) \n",
    "graph.render(\"tree graph\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Discuss the results seen for the two trees\n",
    "\n",
    "Discuss the results you have just seen. What do the various accuracy-score values tell you? How do the two trees that are produced differ? Looking at the outputs (leaves) of the simplified-data tree, what sorts of errors does that tree make?\n",
    "\n",
    "First, while the accuracy score for training data and testing data are relatively the same for the three-class-classification, when using the decision tree on the full data set, it performs poorly on the testing data and clearly overfits the training data with an accuracy score of 1. \n",
    "\n",
    "Correspondingly, we see the leaves of the tree for the full data set having the entropy of 0, while the leaves for the tree on smaller dataset have relatively higher entropies. This means there's a tradeoff between the acurracy score and adaptability/generalization of the model, just as seen in the models we encountered in the previous semesters. When the tree perfectly fits to the training data(especially with many classes or features), it's hard to apply it to other data since it becomes very specific. \n",
    "\n",
    "On the other hand, for the smaller dataset, because we preprocessed the data, and the range of the outcomes(rings) is narrowed to 3 classes, the tree becomes a lot simpler. The leaves of the simplified-data tree often have high entropy. When we color-fill and label the tree, all classes are either small or medium, which means that no features are able to separate the “large” class. The problem could be due to our dataset or the given features — either the dataset is heavily skewed and there are thousands of small/medium data and only 27 large data, or that we don’t have enough features or the hard threshold at 0.5 doesn’t have the power to successfully separate the dataset.  Either way, although the performance is relatively consistent over training and testing dataset, the errors manifests as relatively high entropy in the leaf nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
