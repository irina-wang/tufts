{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name and ID\n",
    "\n",
    "Irina Mengqi Wang\n",
    "1278675 / mwang17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW04 Code\n",
    "\n",
    "\n",
    "You will complete the following notebook, as described in the PDF for Homework 04 (included in the download with the starter code).  You will submit:\n",
    "1. This notebook file, along with your COLLABORATORS.txt file, to the Gradescope link for code.\n",
    "2. A PDF of this notebook and all of its output, once it is completed, to the Gradescope link for the PDF.\n",
    "\n",
    "\n",
    "Please report any questions to the [class Piazza page](piazza.com/tufts/spring2021/comp135).\n",
    "\n",
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from MLPClassifierWithSolverLBFGS import MLPClassifierLBFGS\n",
    "\n",
    "from viz_tools_for_binary_classifier import plot_pretty_probabilities_for_clf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_tr_N2 = np.loadtxt('data_xor/x_train.csv', skiprows=1, delimiter=',')\n",
    "x_te_N2 = np.loadtxt('data_xor/x_test.csv', skiprows=1, delimiter=',')\n",
    "\n",
    "y_tr_N = np.loadtxt('data_xor/y_train.csv', skiprows=1, delimiter=',')\n",
    "y_te_N = np.loadtxt('data_xor/y_test.csv', skiprows=1, delimiter=',')\n",
    "\n",
    "assert x_tr_N2.shape[0] == y_tr_N.shape[0]\n",
    "assert x_te_N2.shape[0] == y_te_N.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: MLP size [2] with activation ReLU and L-BFGS solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit this block to run from 16 different random_states\n",
    "# Save each run's trained classifier object in a list\n",
    "\n",
    "n_runs = 16\n",
    "start_time_sec = time.time()\n",
    "mlp_lbfgs = MLPClassifierLBFGS(\n",
    "    hidden_layer_sizes=[2],\n",
    "    activation='relu',\n",
    "    alpha=0.0001,\n",
    "    max_iter=200, tol=1e-6,\n",
    "    random_state=0,\n",
    "    )\n",
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "    mlp_lbfgs.fit(x_tr_N2, y_tr_N)\n",
    "elapsed_time_sec = time.time() - start_time_sec\n",
    "print('finished LBFGS run %2d/%d after %6.1f sec | %3d iters | %s | loss %.3f' % (\n",
    "    1, n_runs, elapsed_time_sec,\n",
    "    len(mlp_lbfgs.loss_curve_),\n",
    "    'converged   ' if mlp_lbfgs.did_converge else 'NOT converged',\n",
    "    mlp_lbfgs.loss_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 (a): Visualize probabilistic predictions in 2D feature space for ReLU + L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_grid = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n",
    "plot_pretty_probabilities_for_clf(mlp_lbfgs, x_tr_N2, y_tr_N, ax=ax_grid[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 (b): What fraction of runs reach 0 training error? What happens to the others? Describe how rapidly (or slowly) things seem to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: MLP size [2] with activation Logistic and L-BFGS solver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit this block to run 16 different random_state models with LOGISTIC activation\n",
    "\n",
    "# Save each run's trained classifier object in a list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 (a): Visualize probabilistic predictions in 2D feature space for Logistic Sigmoid + L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_grid = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n",
    "plot_pretty_probabilities_for_clf(mlp_lbfgs, x_tr_N2, y_tr_N, ax=ax_grid[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 (b): What fraction of runs reach 0 training error? What happens to the others? Describe how rapidly (or slowly) things seem to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: MLP size [2] with activation ReLU and SGD solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit this block to do 16 different runs (each with different random_state value)\n",
    "# Save each run's trained classifier object in a list \n",
    "\n",
    "n_runs = 16\n",
    "start_time_sec = time.time()\n",
    "mlp_sgd = MLPClassifier(\n",
    "    hidden_layer_sizes=[2],\n",
    "    activation='relu',\n",
    "    alpha=0.0001,\n",
    "    max_iter=400, tol=1e-8,\n",
    "    random_state=0,\n",
    "    solver='sgd', batch_size=10,\n",
    "    learning_rate='adaptive', learning_rate_init=0.1, momentum=0.0,\n",
    "    )\n",
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "    mlp_sgd.fit(x_tr_N2, y_tr_N)\n",
    "mlp_sgd.did_converge = True if len(warn_list) == 0 else False\n",
    "elapsed_time_sec = time.time() - start_time_sec\n",
    "print('finished SGD run %2d/%d after %6.1f sec | %3d epochs | %s | loss %.3f' % (\n",
    "    1, n_runs, elapsed_time_sec,\n",
    "    len(mlp_sgd.loss_curve_),\n",
    "        'converged    ' if mlp_sgd.did_converge else 'NOT converged',\n",
    "        mlp_sgd.loss_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 (a): Visualize probabilistic predictions in 2D feature space for ReLU + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit to plot all 16 runs from above\n",
    "\n",
    "fig, ax_grid = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n",
    "plot_pretty_probabilities_for_clf(mlp_sgd, x_tr_N2, y_tr_N, ax=ax_grid[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 (b): What fraction of runs reach 0 training error? What happens to the others? Describe how rapidly (or slowly) things seem to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 (c): What is most noticeably different between SGD with batch size 10 and the previous L-BFGS in part 1 (using the same ReLU activation function)?  Why, do you believe, these differences exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: MLP size [2] with activation Logistic and SGD solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit to do 16 runs of SGD, like in previous step, but with LOGISTIC activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4(a): Visualize probabilistic predictions in 2D feature space for Logistic + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit to plot all 16 runs from previous step\n",
    "\n",
    "fig, ax_grid = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n",
    "plot_pretty_probabilities_for_clf(mlp_sgd, x_tr_N2, y_tr_N, ax=ax_grid[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 (b): What fraction of runs reach 0 training error? What happens to the others? Describe how rapidly (or slowly) things seem to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 (c): What is most noticeably different between SGD with batch size 10 and the previous L-BFGS runs in part 2 (using the same logistic activation function)?  Why, do you believe, these differences exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Comparing loss_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 (a): Plot loss_curves for each method in 2 x 2 subplot grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_grid = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True, figsize=(8,5))\n",
    "\n",
    "# TODO plot 16 curves for each of the 2x2 settings of solver and activation\n",
    "ax_grid[0,0].set_title('L-BFGS ReLU')\n",
    "ax_grid[0,1].set_title('L-BFGS Logistic')\n",
    "    \n",
    "ax_grid[1,0].set_title('SGD ReLU')\n",
    "ax_grid[1,1].set_title('SGD Logistic')\n",
    "plt.ylim([0, 1.0]); # keep this y limit so it's easy to compare across plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 (b): From this overview plot (plus your detailed plots from prior steps), which activation function seems easier to optimize, the ReLU or the Logistic Sigmoid?  Which requires most iterations in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 (c): Are you convinced that one activation function is always easier to optimize? Suggest 3 additional experimental comparisons that would be informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
