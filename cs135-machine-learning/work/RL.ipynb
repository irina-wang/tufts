{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distributed-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import random \n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-protection",
   "metadata": {},
   "source": [
    "## MDP ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "private-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIFF_WORLD = np.array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                       [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                       [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                       [0, 1, 1, 1, 1, 1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "secondary-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN = 0\n",
    "CLIFF = 1\n",
    "\n",
    "ROWS = range(len(CLIFF_WORLD))\n",
    "COLS = range(len(CLIFF_WORLD[0]))\n",
    "TOP_EDGE = 0\n",
    "BOTTOM_EDGE = len(CLIFF_WORLD)-1\n",
    "LEFT_EDGE = 0\n",
    "RIGHT_EDGE = len(CLIFF_WORLD[0])-1\n",
    "\n",
    "\n",
    "STATE_START = (BOTTOM_EDGE, LEFT_EDGE)\n",
    "STATE_GOAL = (BOTTOM_EDGE, RIGHT_EDGE)\n",
    "\n",
    "REWARD_BASIC = -1\n",
    "REWARD_CLIFF = -100\n",
    "\n",
    "class Action(Enum):\n",
    "    UP = 1 \n",
    "    DOWN = 2 \n",
    "    LEFT = 3 \n",
    "    RIGHT = 4 \n",
    "    \n",
    "    \n",
    "def state_transition(state, action):\n",
    "    # absorbing state \n",
    "    if state == STATE_GOAL: \n",
    "        return state, 0\n",
    "    \n",
    "    row = state[0]\n",
    "    col = state[1]\n",
    "    if action is Action.UP and state[0] != TOP_EDGE:\n",
    "        row = state[0] - 1\n",
    "    elif action is Action.DOWN and state[0] != BOTTOM_EDGE:\n",
    "        row = state[0] + 1\n",
    "    elif action is Action.LEFT and state[1] != LEFT_EDGE:\n",
    "        col = state[1] - 1\n",
    "    elif action is Action.RIGHT and state[1] != RIGHT_EDGE:\n",
    "        col = state[1] + 1\n",
    "        \n",
    "    next_state = (row,col)\n",
    "    \n",
    "    reward = REWARD_BASIC\n",
    "    \n",
    "    if CLIFF_WORLD[row][col] == CLIFF:\n",
    "        next_state = STATE_START\n",
    "        reward = REWARD_CLIFF\n",
    "        \n",
    "    return next_state, reward "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-nurse",
   "metadata": {},
   "source": [
    "## learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sorted-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.1\n",
    "GAMMA = 0.9\n",
    "EPSILON = 0.1 \n",
    "EPISODE_NUM = 500\n",
    "EPISODE_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vertical-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_select(state, q_values, epsilon=0):\n",
    "    action_selected = random.choice(list(Action))\n",
    "    if random.random() >= epsilon:\n",
    "        actions = [action for action in Action]\n",
    "        qvals = [q_values.get((state, action), float('-inf')) for action in actions]\n",
    "        # ties : choose the first action \n",
    "        action_selected = actions[np.argmax(qvals)]\n",
    "        \n",
    "    return action_selected\n",
    "    \n",
    "def generate_greedy_policy(q_values):\n",
    "    policy = np.full_like(CLIFF_WORLD, Action.UP, dtype = Action)\n",
    "    \n",
    "    for row in ROWS:\n",
    "        for col in COLS:\n",
    "            policy[row][col] = epsilon_greedy_select((row,col), q_values, epsilon=0)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def print_policy(policy):\n",
    "    def name_action(row, col):\n",
    "        action_name = policy[row][col].name[0]\n",
    "        if CLIFF_WORLD[row][col] == CLIFF:\n",
    "            action_name = '_'\n",
    "        elif (row,col) == STATE_GOAL:\n",
    "            action_name = 'G'\n",
    "        return action_name\n",
    "    \n",
    "    for row in ROWS:\n",
    "        row_str = [name_action(row,col) for col in COLS]\n",
    "        print(''.join(row_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-motivation",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "approximate-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(epsilon):\n",
    "    q_values = {}\n",
    "    for episode in range(EPISODE_NUM):\n",
    "        state = STATE_START\n",
    "        actiion = epsilon_greedy_select(state, q_values, epsilon)\n",
    "        for iteration in range(EPISODE_LENGTH):\n",
    "            next_state, reward = state_transition(state, action)\n",
    "            next_action = epsilon_greedy_select(next_state, q_values, epsilon)\n",
    "            qval = q_values.get((state,action), 0)\n",
    "            next_qval = q_values.get((next_state,next_action), 0)\n",
    "            qval = qval + ALPHA * (reward + GAMMA * next_qval - qval)\n",
    "            \n",
    "            q_values[(state, action)] = qval\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action \n",
    "    return q_values, generate_greedy_policy(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hairy-problem",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'action' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3b482b0bef12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_final_sarsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_sarsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msarsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_sarsa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-5d5928abc2cc>\u001b[0m in \u001b[0;36msarsa\u001b[0;34m(epsilon)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mactiion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mnext_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mqval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'action' referenced before assignment"
     ]
    }
   ],
   "source": [
    "q_final_sarsa, policy_sarsa = sarsa(EPSILON)\n",
    "print_policy(policy_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-baseline",
   "metadata": {},
   "source": [
    "### Q-learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-setting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
