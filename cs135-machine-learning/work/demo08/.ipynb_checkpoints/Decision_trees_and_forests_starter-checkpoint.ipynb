{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hourly-karma",
   "metadata": {},
   "source": [
    "## Basic use of decision trees and ensembles (forests)\n",
    "We can classify data based upon features in a number of ways.  While many such methods use weights applied to those features, *decision trees* instead treat features as variables controlling a branching path through the data, selecting subsets based upon the values of their features, and seeking to classify that data given particular combinations of feature-values.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "A *decision forest* is a combination of multiple such trees.  Such classifiers use the combined outputs of each tree in the forest—in a basic \"voting\" scheme—in order to classify new data.  Each tree in the forest is designed to be somewhat different, usually by using different parts of the training data, different subsets of the overall set of features, and the like.  As a result, decision forests often show better performance with respect to overfitting, since they already introduce a lot of model diversity.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "precious-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(print_changed_only=False)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collected-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A synthetic data set of 10 thousand entries, where only some of the features are likely \n",
    "# to be of practical use in classification.\n",
    "num_features = 25\n",
    "num_inform = 5\n",
    "x, y = make_classification(n_samples=10_000, n_features=num_features, n_informative=num_inform, \n",
    "                           n_redundant=(num_features - num_inform), random_state=13)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-shirt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
