{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "formed-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unnecessary-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "x_train_df = pd.read_csv('../data/data_reviews/x_train.csv')\n",
    "y_train_df = pd.read_csv('../data/data_reviews/y_train.csv')\n",
    "x_test_df = pd.read_csv('../data/data_reviews/x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wanted-attachment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Oh and I forgot to also mention the weird colo...\n",
       "1                            THAT one didn't work either.\n",
       "2                                      Waste of 13 bucks.\n",
       "3       Product is useless, since it does not have eno...\n",
       "4       None of the three sizes they sent with the hea...\n",
       "                              ...                        \n",
       "2395    The sweet potato fries were very good and seas...\n",
       "2396    I could eat their bruschetta all day it is dev...\n",
       "2397                                 Ambience is perfect.\n",
       "2398    We ordered the duck rare and it was pink and t...\n",
       "2399         Service was good and the company was better!\n",
       "Name: text, Length: 2400, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train_df['text'] \n",
    "x_test = x_test_df['text']\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "israeli-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_i = np.where(x_train_df['website_name']=='amazon')\n",
    "imdb_i = np.where(x_train_df['website_name']=='imdb')\n",
    "yelp_i =  np.where(x_train_df['website_name']=='yelp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-registration",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coral-fossil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: movie:film -> theater:____\n",
      "                theatre  at dist 1.977\n",
      "                theater  at dist 2.158\n",
      "                 cinema  at dist 3.639\n",
      "                  opera  at dist 3.654\n",
      "                 ballet  at dist 3.729\n",
      "               ensemble  at dist 3.817\n",
      "                 studio  at dist 3.967\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.neighbors\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "word_embeddings = pd.read_csv('../data/pretrained_word_embeddings/glove.6B.50d.txt.zip',\n",
    "                               header=None, sep=' ', index_col=0,\n",
    "                               nrows=100000, compression='zip', encoding='utf-8', quoting=3)\n",
    "# Build a dict that will map from string word to 50-dim vector\n",
    "word_list = word_embeddings.index.values.tolist()\n",
    "word2vec = OrderedDict(zip(word_list, word_embeddings.values))\n",
    "\n",
    "## Show some examples\n",
    "\n",
    "\n",
    "n_words = len(word2vec.keys())\n",
    "# print(\"word2vec['happy'] = \")\n",
    "# print(word2vec['happy'])\n",
    "\n",
    "# print(\"word2vec['good'] = \")\n",
    "# print(word2vec['good'])\n",
    "\n",
    "## Try some analogies\n",
    "def analogy_lookup(a1, a2, b1):\n",
    "    target_vec = word2vec[a2] - word2vec[a1] + word2vec[b1]\n",
    "    knn = sklearn.neighbors.NearestNeighbors(n_neighbors=7, metric='euclidean', algorithm='brute')\n",
    "    knn.fit(word_embeddings.values)\n",
    "    dists, indices = knn.kneighbors(target_vec[np.newaxis,:])\n",
    "    print(\"Query: %s:%s -> %s:____\" % (a1, a2, b1))\n",
    "    for ii, vv in enumerate(indices[0]):\n",
    "        print(\"   %20s  at dist %.3f\" % (word_list[vv], dists[0,ii]))\n",
    "\n",
    "# analogy_lookup('england', 'london', 'france')\n",
    "# analogy_lookup('england', 'london', 'germany')\n",
    "# analogy_lookup('england', 'london', 'japan')\n",
    "# analogy_lookup('england', 'london', 'indonesia')\n",
    "\n",
    "# analogy_lookup('swim', 'swimming', 'run')\n",
    "\n",
    "\n",
    "analogy_lookup('movie', 'film', 'theater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nearby-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def simple_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z]\", \" \", str_input).lower().split()   \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "satisfactory-geneva",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28937"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [simple_tokenizer(x_train[i]) for i in range(len(x_train))]\n",
    "\n",
    "flat_list = sum(words, [])\n",
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-dubai",
   "metadata": {},
   "source": [
    "## Taking the Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "statewide-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = word2vec.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "independent-window",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-be1089292c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml135_env/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml135_env/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mrcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;31m# Make this warning show up first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml135_env/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mitems\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "x_tr = []\n",
    "\n",
    "for x in range(len(words)):\n",
    "    for i in range(len(words[x])):\n",
    "        filtered = []\n",
    "        if (words[x][i] in keys):\n",
    "            filtered.append(words[x][i])\n",
    "        t = [word2vec[w] for w in filtered]\n",
    "    x_tr.append(np.average(t, axis=1))\n",
    "x = np.array(x_tr, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-broadway",
   "metadata": {},
   "source": [
    "**Note: tense, persons**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-marriage",
   "metadata": {},
   "source": [
    "# Logistic Regression #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "robust-smell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x\n",
    "y = y_train_df['is_positive_sentiment'].to_numpy()\n",
    "feat_num = X.shape\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "written-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-training",
   "metadata": {},
   "source": [
    "### CV without best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "magnetic-berry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished SGD run  1/16 after   30.0 sec | 400 epochs | NOT converged | loss 0.023\n",
      "finished SGD run  2/16 after   30.9 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run  3/16 after   32.0 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run  4/16 after   32.5 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run  5/16 after   30.5 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run  6/16 after   32.1 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run  7/16 after   31.3 sec | 400 epochs | NOT converged | loss 0.021\n",
      "finished SGD run  8/16 after   31.6 sec | 400 epochs | NOT converged | loss 0.023\n",
      "finished SGD run  9/16 after   30.8 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run 10/16 after   30.7 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run 11/16 after   31.0 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run 12/16 after   32.4 sec | 400 epochs | NOT converged | loss 0.023\n",
      "finished SGD run 13/16 after   31.2 sec | 400 epochs | NOT converged | loss 0.020\n",
      "finished SGD run 14/16 after   31.9 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run 15/16 after   34.5 sec | 400 epochs | NOT converged | loss 0.020\n",
      "finished SGD run 16/16 after   29.2 sec | 400 epochs | NOT converged | loss 0.022\n"
     ]
    }
   ],
   "source": [
    "n_runs = 16\n",
    "tr_classifierLOG_SGD = list()\n",
    "\n",
    "for i in range(n_runs):\n",
    "    start_time_sec = time.time()\n",
    "    mlp_sgd = MLPClassifier(\n",
    "        hidden_layer_sizes=[2],\n",
    "        activation='logistic',\n",
    "        alpha=0.0001,\n",
    "        max_iter=400, tol=1e-8,\n",
    "        random_state=i,\n",
    "        solver='sgd', batch_size=10,\n",
    "        learning_rate='adaptive', learning_rate_init=0.1, momentum=0.0,\n",
    "        )\n",
    "    with warnings.catch_warnings(record=True) as warn_list:\n",
    "        clf = mlp_sgd.fit(X, y)\n",
    "    mlp_sgd.did_converge = True if len(warn_list) == 0 else False\n",
    "    elapsed_time_sec = time.time() - start_time_sec\n",
    "    print('finished SGD run %2d/%d after %6.1f sec | %3d epochs | %s | loss %.3f' % (\n",
    "        i+1, n_runs, elapsed_time_sec,\n",
    "        len(mlp_sgd.loss_curve_),\n",
    "            'converged    ' if mlp_sgd.did_converge else 'NOT converged',\n",
    "            mlp_sgd.loss_))\n",
    "\n",
    "    tr_classifierLOG_SGD.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------\\nClassify with base data, 5 folds\\n-----------------\")\n",
    "\n",
    "k = 3\n",
    "kfold = KFold(n_splits=k)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "# model = LogisticRegression()\n",
    "\n",
    "for train_idx, test_idx in kfold.split(X):\n",
    "    X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    \n",
    "    mlp_sgd = MLPClassifier(\n",
    "            hidden_layer_sizes=[2],\n",
    "            activation='logistic',\n",
    "            alpha=0.0001,\n",
    "            max_iter=400, tol=1e-8,\n",
    "            random_state=i,\n",
    "            solver='sgd', batch_size=10,\n",
    "            learning_rate='adaptive', learning_rate_init=0.1, momentum=0.0,\n",
    "            )\n",
    "        with warnings.catch_warnings(record=True) as warn_list:\n",
    "            clf = mlp_sgd.fit(X_train, y_train)\n",
    "            pred_train = clf.predict_proba(X_train)\n",
    "            pred_test = clf.predict_proba(X_test)\n",
    "            \n",
    "            score_train = clf.score(X_train, y_train)\n",
    "            score_test = clf.score(X_test, y_test)\n",
    "        mlp_sgd.did_converge = True if len(warn_list) == 0 else False\n",
    "        elapsed_time_sec = time.time() - start_time_sec\n",
    "        print('finished SGD run after %6.1f sec | %3d epochs | %s | loss %.3f' % (\n",
    "              elapsed_time_sec,\n",
    "            len(mlp_sgd.loss_curve_),\n",
    "                'converged    ' if mlp_sgd.did_converge else 'NOT converged',\n",
    "                mlp_sgd.loss_))\n",
    "\n",
    "        tr_classifierLOG_SGD.append(clf)\n",
    "    \n",
    "\n",
    " \n",
    "    \n",
    "    train_scores.append(score_train)\n",
    "    test_scores.append(score_test)\n",
    "    \n",
    "print(\"\\nAverage train accuracy: \", np.average(score_train))\n",
    "print(\"Average test accuracy: \", np.average(score_test))\n",
    "\n",
    "\n",
    "    n_runs = 16\n",
    "    tr_classifierLOG_SGD = list()\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        start_time_sec = time.time()\n",
    "        mlp_sgd = MLPClassifier(\n",
    "            hidden_layer_sizes=[2],\n",
    "            activation='logistic',\n",
    "            alpha=0.0001,\n",
    "            max_iter=400, tol=1e-8,\n",
    "            random_state=i,\n",
    "            solver='sgd', batch_size=10,\n",
    "            learning_rate='adaptive', learning_rate_init=0.1, momentum=0.0,\n",
    "            )\n",
    "        with warnings.catch_warnings(record=True) as warn_list:\n",
    "            clf = mlp_sgd.fit(X, y)\n",
    "        mlp_sgd.did_converge = True if len(warn_list) == 0 else False\n",
    "        elapsed_time_sec = time.time() - start_time_sec\n",
    "        print('finished SGD run %2d/%d after %6.1f sec | %3d epochs | %s | loss %.3f' % (\n",
    "            i+1, n_runs, elapsed_time_sec,\n",
    "            len(mlp_sgd.loss_curve_),\n",
    "                'converged    ' if mlp_sgd.did_converge else 'NOT converged',\n",
    "                mlp_sgd.loss_))\n",
    "\n",
    "        tr_classifierLOG_SGD.append(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-sally",
   "metadata": {},
   "source": [
    "### CV with the best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_grid = np.logspace(-9, 6, 31)\n",
    "model_list = []\n",
    "aver_train_score = []\n",
    "aver_test_score = []\n",
    "aver_train_loss = []\n",
    "aver_test_loss = []\n",
    "\n",
    "\n",
    "for C in C_grid:\n",
    "    k = 3\n",
    "    kfold = KFold(n_splits=k)\n",
    "    \n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "\n",
    "    model = sklearn.linear_model.LogisticRegression(C=C,solver='liblinear')\n",
    "    for train_idx, test_idx in kfold.split(X):\n",
    "        X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        pred_train = model.predict_proba(X_train)\n",
    "        pred_test = model.predict_proba(X_test)\n",
    "\n",
    "        # Score\n",
    "        score_train = model.score(X_train, y_train)\n",
    "        score_test = model.score(X_test, y_test)\n",
    "#         print(\"Train score: \", score_train)\n",
    "#         print(\"Test score: \", score_test)\n",
    "        train_scores.append(score_train)\n",
    "        test_scores.append(score_test)\n",
    "        \n",
    "        # Log loss\n",
    "        log_loss_train = sklearn.metrics.log_loss(y_train,pred_train)\n",
    "        log_loss_test = sklearn.metrics.log_loss(y_test,pred_test)\n",
    "#         print(\"Train loss: \", log_loss_train)\n",
    "#         print(\"Test loss: \", log_loss_test)\n",
    "        train_loss.append(log_loss_train)\n",
    "        test_loss.append(log_loss_test)\n",
    "        \n",
    "        \n",
    "    print(\"\\nFor C value : \", C)\n",
    "    print(\"\\nAverage train accuracy: \", np.average(score_train))\n",
    "    print(\"Average test accuracy: \", np.average(score_test))\n",
    "    print(\"\\nAverage train loss: \", np.average(train_loss))\n",
    "    print(\"Average test loss: \", np.average(test_loss))\n",
    "    \n",
    "    print('------------------------------------------------\\n')\n",
    "    \n",
    "    model_list.append(model)\n",
    "    aver_train_score.append(np.average(score_train))\n",
    "    aver_test_score.append(np.average(score_test))\n",
    "    aver_train_loss.append(np.average(train_loss))\n",
    "    aver_test_loss.append(np.average(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the results in clear tabular format\n",
    "pd.DataFrame(np.transpose([aver_train_score, aver_test_score, aver_train_loss, aver_test_loss]), columns=['train accuracy', 'test accuracy', 'train loss', 'test loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-quest",
   "metadata": {},
   "source": [
    "#### Best Log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = min(aver_test_loss)\n",
    "index_N2 = aver_test_loss.index(min_loss)\n",
    "best_C =  C_grid[index_N2]\n",
    "best_model = model_list[index_N2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-monday",
   "metadata": {},
   "source": [
    "#### stability across Kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------\\nClassify with base data, 5 folds\\n-----------------\")\n",
    "\n",
    "K = [2,3,4,5,6,7,8,9,10,11,12]\n",
    "K_train_loss = []\n",
    "K_test_loss = []\n",
    "for k in K:\n",
    "    kfold = KFold(n_splits=k)\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for train_idx, test_idx in kfold.split(X):\n",
    "        shuffler = np.random.permutation(len(X))\n",
    "        X_shuffled = X[shuffler]\n",
    "        y_shuffled = y[shuffler]\n",
    "        X_train, X_test = X_shuffled[train_idx,:], X_shuffled[test_idx,:]\n",
    "        y_train, y_test = y_shuffled[train_idx], y_shuffled[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        pred_train = best_model.predict_proba(X_train)\n",
    "        pred_test = best_model.predict_proba(X_test)\n",
    "\n",
    "        score_train = best_model.score(X_train, y_train)\n",
    "        score_test = best_model.score(X_test, y_test)\n",
    "        train_scores.append(score_train)\n",
    "        test_scores.append(score_test)\n",
    "        \n",
    "        log_loss_train = sklearn.metrics.log_loss(y_train,pred_train)\n",
    "        log_loss_test = sklearn.metrics.log_loss(y_test,pred_test)\n",
    "        \n",
    "        train_loss.append(log_loss_train)\n",
    "        test_loss.append(log_loss_test)\n",
    "\n",
    "    print(\"\\nAverage train accuracy: \", np.average(score_train))\n",
    "    print(\"Average test accuracy: \", np.average(score_test))\n",
    "    print(\"Average train loss: \", np.average(train_loss))\n",
    "    print(\"Average test loss: \", np.average(test_loss))\n",
    "    \n",
    "    K_train_loss.append(np.average(train_loss))\n",
    "    K_test_loss.append(np.average(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('K from 2 to 12');\n",
    "plt.ylabel('logistic loss');\n",
    "\n",
    "sns.lineplot(x = K, y = K_train_loss, label = \"Train Loss\", color = \"red\", marker='o')\n",
    "sns.lineplot(x = K, y = K_test_loss, label = \"Test Loss\", color = \"blue\", marker='o')\n",
    "\n",
    "# show a legend on the plot \n",
    "plt.legend() \n",
    "plt.title('Log loss across K values')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('standard deviation for training set: %.3f  ' %np.std(K_train_loss))\n",
    "print('standard deviation for testing set: %.3f  ' %np.std(K_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('log10(C)');\n",
    "plt.ylabel('logistic loss');\n",
    "plt.ylim([0.0, 1]);\n",
    "\n",
    "sns.lineplot(x = np.log10(C_grid), y = aver_train_loss, label = \"Train Loss\", color = \"red\", marker='o')\n",
    "sns.lineplot(x = np.log10(C_grid), y = aver_test_loss,label = \"Test Loss\", color = \"blue\", marker='o')\n",
    "\n",
    "# show a legend on the plot \n",
    "plt.legend() \n",
    "plt.title('Logistic loss on C-grid')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best C-value for LR: %.3f\" % best_C) \n",
    "print(\"Test set log-loss at best C-value: %.4f\" % min_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-satin",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "yproba1_test = best_model.predict_proba(x_te)[:, 1] \n",
    "np.savetxt('yproba1_test.txt', yproba1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-circuit",
   "metadata": {},
   "source": [
    "# Neuronetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-paste",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 16\n",
    "tr_classifierLBFS = list()\n",
    "\n",
    "for i in range(n_runs):\n",
    "    start_time_sec = time.time()\n",
    "    mlp_lbfgs = MLPClassifier(\n",
    "        hidden_layer_sizes=[2],\n",
    "        activation='relu',\n",
    "        alpha=0.0001,\n",
    "        max_iter=200, tol=1e-6,\n",
    "        random_state=i,\n",
    "        )\n",
    "    with warnings.catch_warnings(record=True) as warn_list:\n",
    "        clf = mlp_lbfgs.fit(x_tr_N2, y_tr_N)\n",
    "    elapsed_time_sec = time.time() - start_time_sec\n",
    "    print('finished LBFGS run %2d/%d after %6.1f sec | %3d iters | %s | loss %.3f' % (\n",
    "        i+1, n_runs, elapsed_time_sec,\n",
    "        len(mlp_lbfgs.loss_curve_),\n",
    "        'converged   ' if mlp_lbfgs.did_converge else 'NOT converged',\n",
    "        mlp_lbfgs.loss_))\n",
    "        \n",
    "    tr_classifierLBFS.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-commodity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
