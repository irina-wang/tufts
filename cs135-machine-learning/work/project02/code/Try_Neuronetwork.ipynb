{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "formed-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unnecessary-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "x_train_df = pd.read_csv('../data/data_reviews/x_train.csv')\n",
    "y_train_df = pd.read_csv('../data/data_reviews/y_train.csv')\n",
    "x_test_df = pd.read_csv('../data/data_reviews/x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wanted-attachment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Technically, the film is well made with impres...\n",
       "1      !....THE OWNERS REALLY REALLY need to quit bei...\n",
       "2                                  what a disappointment\n",
       "3              The movie is terribly boring in places.  \n",
       "4      One of the best mexican movies ever!, and one ...\n",
       "                             ...                        \n",
       "595      This is a great restaurant at the Mandalay Bay.\n",
       "596    I could care less... The interior is just beau...\n",
       "597    The only consistent thread holding the series ...\n",
       "598    My side Greek salad with the Greek dressing wa...\n",
       "599    However, my recent experience at this particul...\n",
       "Name: text, Length: 600, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train_df['text'] \n",
    "x_test = x_test_df['text']\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "israeli-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_i = np.where(x_train_df['website_name']=='amazon')\n",
    "imdb_i = np.where(x_train_df['website_name']=='imdb')\n",
    "yelp_i =  np.where(x_train_df['website_name']=='yelp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-registration",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advised-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-architecture",
   "metadata": {},
   "source": [
    "#### simple processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "x = count_vectorizer.fit_transform(x_train)\n",
    "print('Num of feat: ', len(x.toarray()[0]))\n",
    "print(count_vectorizer.get_feature_names())\n",
    "print(x.toarray())\n",
    "pd.DataFrame(x.toarray(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-jordan",
   "metadata": {},
   "source": [
    "### Build your own tokenizer ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "compressed-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def simple_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z]\", \" \", str_input).lower().split()\n",
    "    \n",
    "#     def prune_food(w):\n",
    "#         if w == 'bones' or w == 'bone' or w == 'fish' or w == 'worms' or w == 'worm':\n",
    "#             w = 'food'\n",
    "#         return w\n",
    "    \n",
    "#     words = [prune_food(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english',tokenizer=simple_tokenizer)\n",
    "x = count_vectorizer.fit_transform(x_train)\n",
    "print('Num of feat: ', len(x.toarray()[0]))\n",
    "print(count_vectorizer.get_feature_names())\n",
    "print(x.toarray())\n",
    "pd.DataFrame(x.toarray(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "formal-quarterly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/mac/.local/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: tqdm in /Users/mac/opt/anaconda3/envs/ml135_env/lib/python3.8/site-packages (from nltk) (4.60.0)\n",
      "Requirement already satisfied: regex in /Users/mac/opt/anaconda3/envs/ml135_env/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in /Users/mac/opt/anaconda3/envs/ml135_env/lib/python3.8/site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: click in /Users/mac/opt/anaconda3/envs/ml135_env/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "logical-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "porterstemmer = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "located-gothic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-098446e4a95e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# nltk.tag.pos_tag(\"I am named John Doe\".split())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mTKINTER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                 \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1934\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1936\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml135_env/lib/python3.8/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "# nltk.tag.pos_tag(\"I am named John Doe\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ongoing-empire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "busi\n",
      "bu\n",
      "busi\n",
      "busi\n"
     ]
    }
   ],
   "source": [
    "# same stem \n",
    "print(porterstemmer.stem('business'))\n",
    "print(porterstemmer.stem('bus'))\n",
    "print(porterstemmer.stem('businesses'))\n",
    "print(porterstemmer.stem('busy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-survey",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "oriental-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "import re\n",
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split()\n",
    "\n",
    "    # prune words\n",
    "#     def prune_food(w):\n",
    "#         if w == 'bones' or w == 'bone' or w == 'fish' or w == 'worms' or w == 'worm':\n",
    "#             w = 'food'\n",
    "#         return w\n",
    "    \n",
    "#     words = [prune_food(word) for word in words]\n",
    "    \n",
    "    # stemming \n",
    "    porter_stemmer = PorterStemmer()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    \n",
    "    #remove non important words\n",
    "    non_important = ['film', 'movie','apple', 'juice' ]\n",
    "    words = [w for w in words if w not in non_important]\n",
    "    \n",
    "    #     stops = stopwords.words('english')\n",
    "    #     words = [w for w in words if w not in stops]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-broadway",
   "metadata": {},
   "source": [
    "**Note: tense, persons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "under-rider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape (2400, 3620)\n",
      "test set shape (600, 3620)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=stemming_tokenizer)\n",
    "x = count_vectorizer.fit_transform(x_train)\n",
    "x_te = count_vectorizer.transform(x_test)\n",
    "features = count_vectorizer.get_feature_names()\n",
    "\n",
    "print('train set shape', x.shape)\n",
    "print('test set shape', x_te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "romantic-discovery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '--', '-drink', '-good', '-mi', '-year', 'a', 'abandon', 'abhor', 'abil', 'abl', 'abound', 'about', 'abov', 'abroad', 'absolut', 'absolutel', 'absolutley', 'abstrus', 'abysm', 'ac', 'academi', 'accent', 'accept', 'access', 'accessoryon', 'accid', 'accident', 'acclaim', 'accolad', 'accomod', 'accompani', 'accur', 'accus', 'ach', 'achiev', 'achil', 'ackerman', 'acknowledg', 'act', 'acting--even', 'acting-wis', 'action', 'activ', 'actor', 'actress', 'actual', 'ad', 'adapt', 'add', 'addit', 'adhes', 'admin', 'admit', 'ador', 'adrift', 'adventur', 'advertis', 'advis', 'aerial', 'aesthet', 'affect', 'affleck', 'afford', 'afraid', 'africa', 'after', 'afternoon', 'again', 'against', 'age', 'ago', 'agre', 'ahead', 'aimless', 'air', 'airlin', 'akin', 'ala', 'alarm', 'albondiga', 'alexand', 'alik', 'all', 'all-star', 'allot', 'allow', 'almond', 'almost', 'alon', 'along', 'alongsid', 'alot', 'alreadi', 'also', 'although', 'aluminum', 'alway', 'am', 'amateurish', 'amaz', 'amazingli', 'amazon', 'ambianc', 'ambienc', 'america', 'american', 'among', 'amount', 'amp', 'ampl', 'amus', 'an', 'anatomist', 'and', 'andddd', 'angel', 'angela', 'angelina', 'angl', 'angri', 'angu', 'ani', 'anim', 'ann', 'anniversari', 'annoy', 'anoth', 'answer', 'ant', 'antena', 'anthoni', 'anti-glar', 'anticip', 'anymor', 'anyon', 'anyth', 'anytim', 'anyway', 'anywher', 'apart', 'apolog', 'app', 'appal', 'appar', 'appeal', 'appear', 'appet', 'appetit', 'appl', 'applaud', 'applaus', 'appoint', 'appreci', 'appropri', 'approv', 'apt', 'are', 'area', 'aren', 'argu', 'aria', 'armageddon', 'armand', 'armband', 'around', 'array', 'arriv', 'art', 'arti', 'articl', 'articul', 'artist', 'artless', 'as', 'asia', 'asid', 'ask', 'asleep', 'aspect', 'ass', 'assant', 'assault', 'assist', 'assum', 'assur', 'astonishingli', 'astronaut', 'at', 'ate', 'atleast', 'atmospher', 'atroci', 'att', 'attach', 'attack', 'attempt', 'attent', 'attitud', 'attract', 'audienc', 'audio', 'auju', 'aurv', 'austen', 'auster', 'authent', 'author', 'auto', 'auto-answ', 'avail', 'averag', 'avocado', 'avoid', 'aw', 'award', 'away', 'awesom', 'awkward', 'awkwardli', 'awsom', 'ayc', 'aye', 'az', 'b-list', 'baaaaaad', 'baba', 'babbl', 'babi', 'bachi', 'back', 'backdrop', 'background', 'backlight', 'bacon', 'bad', 'bad-ass', 'badli', 'bagel', 'bailey', 'bakeri', 'baklava', 'balanc', 'ball', 'ballet', 'bamboo', 'banana', 'band', 'bank', 'bar', 'barcelona', 'bare', 'bargain', 'bark', 'barney', 'barren', 'bartend', 'base', 'basebal', 'basement', 'basic', 'bat', 'bate', 'bathroom', 'batter', 'batteri', 'baxendal', 'bay', 'bbq', 'be', 'bean', 'bear', 'beat', 'beateou', 'beauti', 'bec', 'becam', 'becaus', 'bechard', 'becom', 'bed', 'beef', 'been', 'been-stepped-in-and-tracked-everywher', 'beep', 'beer', 'befor', 'began', 'begin', 'behe', 'behind', 'behold', 'bela', 'believ', 'bell', 'bellagio', 'belli', 'bellucci', 'belmondo', 'below', 'belt', 'ben', 'bend', 'bennett', 'bergen', 'bertolucci', 'besid', 'best', 'better', 'between', 'bewar', 'beyond', 'bibl', 'big', 'big-budget', 'bigger', 'biggest', 'bill', 'billi', 'bing', 'bipolar', 'bird', 'biscuit', 'bit', 'bitch', 'bitchi', 'bite', 'bitpim', 'black', 'blackberri', 'blacktop', 'blah', 'blake', 'blame', 'bland', 'blandest', 'blandli', 'blare', 'blatant', 'blew', 'bloddi', 'blood', 'bloodi', 'bloodiest', 'blow', 'blown', 'blue', 'blueant', 'bluetoooth', 'bluetooth', 'blush', 'bmw', 'boast', 'bob', 'boba', 'bode', 'boil', 'bold', 'bond', 'bone', 'bonu', 'boob', 'book', 'boost', 'boot', 'bop', 'border', 'bore', 'bose', 'boss', 'both', 'bother', 'bothersom', 'bottom', 'bottowm', 'bouchon', 'bought', 'bougth', 'bowl', 'box', 'boy', 'boyfriend', 'boyl', 'brain', 'brand', 'bread', 'break', 'breakag', 'breakfast', 'breeder', 'breez', 'breviti', 'brian', 'brick', 'brief', 'brigand', 'bright', 'brillianc', 'brilliant', 'brilliantli', 'bring', 'brisket', 'broad', 'broke', 'broken', 'brood', 'brother', 'brought', 'brownish', 'brows', 'browser', 'brunch', 'bruschetta', 'brushfir', 'bt', 'bu', 'buck', 'bud', 'buddi', 'budget', 'buffet', 'build', 'build-up', 'builder', 'built', 'bulki', 'bulli', 'bullock', 'bumper', 'bunch', 'burger', 'burritto', 'burton', 'busi', 'but', 'butter', 'button', 'buy', 'buyer', 'buyer--b', 'buyit', 'buzz', 'by', 'bye', 'c', 'ca-', 'caballero', 'cabl', 'caesar', 'cafe', 'caill', 'cake', 'calamari', 'calendar', 'california', 'call', 'calligraphi', 'came', 'cameo', 'camera', 'camera-work', 'camerawork', 'can', 'can-do', 'canada', 'cancan', 'cancel', 'candac', 'candl', 'cannoli', 'cannot', 'cant', 'capabl', 'cape', 'captain', 'car', 'carb', 'card', 'cardboard', 'cardellini', 'care', 'carli', 'carol', 'carpaccio', 'carrel', 'carri', 'carrier', 'cart', 'cartoon', 'case', 'cash', 'cashew', 'cashier', 'casino', 'cast', 'cat', 'catch', 'catchi', 'caterpillar', 'caught', 'caus', 'cavier', 'cbr', 'cd', 'ceas', 'celebr', 'cell', 'cellphon', 'cellular', 'cent', 'center', 'central', 'centuri', 'certain', 'certainli', 'cg', 'cgi', 'chain', 'chalkboard', 'challeng', 'chanc', 'chance--it', 'chang', 'channel', 'char', 'charact', 'character-ag', 'characteris', 'charcoal', 'charg', 'charge-lif', 'charger', 'charisma', 'charisma-fre', 'charismat', 'charl', 'charli', 'charm', 'chase', 'cheap', 'cheaper', 'cheapli', 'check', 'cheek', 'cheekbon', 'cheerful', 'cheerless', 'chees', 'cheeseburg', 'cheesecurd', 'cheesi', 'chef', 'chemistri', 'chewi', 'chick', 'chicken', 'child', 'child-lik', 'childhood', 'children', 'chilli', 'chimp-lik', 'china', 'chines', 'chip', 'chipolt', 'chipotl', 'chodorov', 'choic', 'choos', 'chosen', 'choux', 'chow', 'christma', 'christoph', 'church', 'cinema', 'cinematographi', 'cinematography-if', 'cingulair', 'cingular', 'circumst', 'claim', 'clariti', 'class', 'classi', 'classic', 'clean', 'clear', 'clearer', 'clearli', 'clever', 'clich', 'click', 'client', 'climax', 'clip', 'clock', 'close', 'close-up', 'cloth', 'club', 'co-star', 'coach', 'coal', 'coaster', 'cocktail', 'coconut', 'cod', 'coffe', 'coher', 'cold', 'colder', 'cole', 'colleagu', 'collect', 'color', 'columbo', 'combin', 'combo', 'come', 'comed', 'comedi', 'comfort', 'comic', 'command', 'comment', 'commentari', 'commerci', 'common', 'commun', 'commut', 'compani', 'companion', 'compar', 'compel', 'compet', 'complain', 'complaint', 'complet', 'complex', 'compliment', 'compos', 'composit', 'comprehens', 'compromis', 'comput', 'con', 'concentr', 'concept', 'conceptu', 'concern', 'concert', 'conclus', 'concret', 'condescend', 'condiment', 'condit', 'confid', 'configur', 'confirm', 'conflict', 'confus', 'connect', 'conneri', 'connisseur', 'connoisseur', 'connor', 'consequ', 'consid', 'consider', 'consist', 'consol', 'constantin', 'constantli', 'construct', 'consum', 'contact', 'contain', 'content', 'continu', 'contract', 'contrast', 'contribut', 'contributori', 'control', 'controversi', 'contstruct', 'conveni', 'convent', 'convers', 'convert', 'convey', 'convinc', 'convolut', 'cook', 'cool', 'copi', 'coppola', 'cord', 'core', 'corn', 'corni', 'correct', 'correctli', 'cost', 'costco', 'costum', 'cotta', 'could', 'couldn', 'couldnt', 'count', 'counterfeit', 'coupl', 'coupon', 'cours', 'court', 'courteou', 'courtroom', 'cover', 'coverag', 'cow', 'cox', 'cozi', 'crab', 'crack', 'crackl', 'cradl', 'craft', 'cram', 'cranberri', 'crap', 'crash', 'crave', 'crawfish', 'crawl', 'crayon', 'crazi', 'creak', 'cream', 'creami', 'creat', 'creativ', 'creatur', 'credibl', 'credit', 'crepe', 'crew', 'cri', 'crisp', 'crispi', 'critic', 'cross', 'crostini', 'crouton', 'crowd', 'crowd-pleaser--thi', 'cruel', 'cruis', 'crumbi', 'crust', 'crusti', 'crystal', 'cuisin', 'cult', 'cultur', 'cumbersom', 'current', 'curri', 'curv', 'custer', 'custom', 'cut', 'cute', 'cutest', 'cutout', 'd', 'dad', 'damian', 'damn', 'danc', 'danger', 'dark', 'darren', 'data', 'date', 'daughter', 'day', 'de', 'dead', 'deadli', 'deadpan', 'deaf', 'deal', 'dealt', 'death', 'debat', 'debbi', 'debit', 'debut', 'decad', 'decay', 'decent', 'decid', 'decis', 'decor', 'dee', 'deep', 'deepli', 'def', 'defect', 'deffinit', 'defin', 'definit', 'definitli', 'del', 'delay', 'delet', 'delici', 'delight', 'delish', 'deliv', 'deliveri', 'denni', 'depend', 'depict', 'depress', 'depth', 'deriv', 'describ', 'descript', 'desert', 'deserv', 'design', 'desir', 'desper', 'despic', 'despis', 'despit', 'dessert', 'destin', 'destroy', 'detach', 'detail', 'deuchebaggeri', 'develop', 'devic', 'devin', 'di', 'diabet', 'dial', 'dialog', 'dialogu', 'diaper', 'dicken', 'did', 'didn', 'die', 'differ', 'difficult', 'dine', 'dinner', 'dip', 'direct', 'direct-to-video', 'director', 'directori', 'dirt', 'dirti', 'dirty-', 'disagre', 'disapoin', 'disapoint', 'disappoint', 'disapppoint', 'disast', 'discard', 'discomfort', 'disconnect', 'discount', 'discov', 'disgrac', 'disgust', 'dish', 'dislik', 'disney', 'dispar', 'dispens', 'display', 'displeas', 'disrespect', 'dissapoint', 'distinct', 'distort', 'distract', 'distress', 'disturb', 'dit', 'divis', 'dna', 'do', 'dock', 'doctor', 'documentari', 'dodg', 'doe', 'doesn', 'dog', 'dollar', 'domin', 'don', 'done', 'donlevi', 'dont', 'doom', 'door', 'dosen', 'doubl', 'doubt', 'douchey', 'dough', 'doughi', 'down', 'download', 'downtown', 'dozen', 'dr', 'dracula', 'draft', 'drag', 'drago', 'drain', 'drama', 'dramat', 'draw', 'dream', 'dreari', 'drench', 'dress', 'dri', 'dribbl', 'driest', 'drift', 'drink', 'drip', 'drive', 'drool', 'drop', 'dual-purpos', 'duck', 'dude', 'due', 'due--th', 'duet', 'dull', 'dumb', 'dumbest', 'duo', 'duper', 'durabl', 'dure', 'duri', 'dust', 'dvd', 'dwight', 'dylan', 'dysfunction--h', 'e', 'each', 'ear', 'earbud', 'earbug', 'eargel', 'earli', 'earlier', 'earpad', 'earphon', 'earpiec', 'earset', 'earth', 'eas', 'easi', 'easier', 'easili', 'eat', 'eaten', 'ebay', 'ebola', 'eccleston', 'echo', 'eclect', 'ed', 'edg', 'edibl', 'edinburgh', 'edit', 'educ', 'edward', 'eel', 'eew', 'effect', 'effici', 'effort', 'egg', 'eggplant', 'eiko', 'either', 'elabor', 'elderli', 'electron', 'eleg', 'elegantli', 'element', 'elia', 'elk', 'eloqu', 'els', 'elsewher', 'email', 'embarass', 'embarrass', 'embassi', 'embed', 'emili', 'emilio', 'emot', 'emperor', 'employe', 'empower', 'empti', 'en', 'enchant', 'encourag', 'end', 'endear', 'endlessli', 'energet', 'energi', 'engag', 'engin', 'english', 'enjoy', 'enough', 'ensu', 'enter', 'enterpris', 'entertain', 'enthusiast', 'entir', 'entre', 'env', 'episod', 'equal', 'equip', 'equival', 'era', 'ergonom', 'ericson', 'ericsson', 'errol', 'error', 'escal', 'escap', 'especi', 'essenti', 'establish', 'estat', 'estevez', 'etc', 'ethic', 'europ', 'european', 'even', 'event', 'eventu', 'ever', 'everi', 'everybodi', 'everyday', 'everyon', 'everyth', 'everything-', 'everywher', 'evid', 'evil', 'evinc', 'exactli', 'exagger', 'exampl', 'excalibur', 'exce', 'exceed', 'excel', 'except', 'excerpt', 'exchang', 'excit', 'exclaim', 'excruciatingli', 'excrutiatingli', 'excus', 'execut', 'exemplar', 'exist', 'existenti', 'expans', 'expect', 'expens', 'experi', 'experienc', 'expert', 'explain', 'explan', 'explor', 'explos', 'express', 'exquisit', 'extant', 'extens', 'exterior', 'extern', 'extra', 'extran', 'extraordinari', 'extrem', 'eye', 'eye-pleas', 'f', 'fabul', 'face', 'facepl', 'facial', 'fact', 'fact-bas', 'factor', 'factori', 'fail', 'fair', 'fairli', 'faith', 'falafel', 'fall', 'fals', 'falwel', 'fame', 'famili', 'familiar', 'famou', 'fan', 'fanci', 'fantasi', 'fantast', 'far', 'farc', 'fare', 'fascin', 'fast', 'faster', 'fat', 'father', 'faultless', 'fausa', 'faux', 'fav', 'favor', 'favorit', 'favourit', 'fear', 'featur', 'feel', 'feel-good', 'feet', 'feisti', 'fell', 'fella', 'fellow', 'felt', 'femal', 'ferri', 'few', 'fianc', 'field', 'fifteen', 'fifti', 'figur', 'file', 'filet', 'fill', 'fillet', 'film--mostli', 'film--someth', 'film-mak', 'filmi', 'filmmak', 'filmographi', 'final', 'find', 'fine', 'finest', 'finger', 'fingernail', 'finish', 'fire', 'firebal', 'firehous', 'first', 'first-person', 'fish', 'fishnet', 'fist', 'fit', 'five', 'flair', 'flake', 'flame', 'flash', 'flashback', 'flat', 'flat-lin', 'flavor', 'flavorless', 'flavour', 'flaw', 'flawless', 'flawlessli', 'flesh', 'fli', 'flick', 'flimsi', 'flip', 'flipphon', 'fliptop', 'floor', 'flop', 'floppi', 'flow', 'flower', 'fluffi', 'flush', 'flynn', 'fm', 'fo', 'focu', 'focus', 'fodder', 'folk', 'follow', 'fondu', 'food', 'fool', 'foot', 'footag', 'footbal', 'for', 'forc', 'ford', 'foreign', 'forev', 'forgeri', 'forget', 'forgett', 'forgot', 'forgotten', 'form', 'format', 'former', 'fort', 'forth', 'forti', 'forward', 'found', 'four', 'fox', 'fraction', 'franc', 'franci', 'francisco', 'free', 'freedom', 'freeman', 'freeway', 'freez', 'french', 'frenchman', 'fresh', 'fri', 'friday', 'friend', 'friendli', 'frighten', 'frog', 'from', 'front', 'frontier', 'frost', 'frozen', 'fruit', 'fs', 'fuck', 'fulci', 'fulfil', 'full', 'fulli', 'fumbl', 'fun', 'function', 'fundament', 'funni', 'further', 'furthermor', 'futur', 'fuzzi', 'fx', 'g', 'ga', 'gabriel', 'gadget', 'galley', 'gallon', 'game', 'ganoush', 'garag', 'garbag', 'garbo', 'garden', 'garlic', 'gaudi', 'gave', 'gay', 'gc', 'geek', 'geeki', 'gel', 'gem', 'gener', 'geniu', 'genr', 'gentle-touch', 'gentli', 'genuin', 'georg', 'gerardo', 'get', 'giallo', 'giant', 'gibberish', 'gimmick', 'giovanni', 'girl', 'girlfriend', 'girolamo', 'give', 'given', 'glad', 'glanc', 'glass', 'glorious', 'glove', 'glu', 'gluten', 'go', 'goat', 'god', 'godfath', 'goe', 'gold', 'golden-crispi', 'gone', 'gonna', 'good', 'gooodd', 'gordon', 'gore', 'goremeist', 'gorman', 'gosh', 'got', 'goth', 'gotta', 'gotten', 'govern', 'grab', 'grace', 'grade', 'gradual', 'grandmoth', 'grant', 'graphic', 'grasp', 'gratitud', 'greas', 'great', 'great--especi', 'greater', 'greatest', 'greek', 'green', 'green-screen', 'greet', 'grew', 'grey', 'grill', 'grim', 'grime', 'grip', 'gristl', 'groceri', 'gross', 'ground', 'ground-break', 'group', 'grow', 'grtting', 'guard', 'guess', 'guest', 'guilt', 'guy', 'gx', 'gyro', 'h', 'ha', 'hackney', 'had', 'hadn', 'hair', 'half', 'halibut', 'ham', 'hand', 'hand-drawn', 'handheld', 'handi', 'handl', 'handmad', 'hands-fre', 'handset', 'handsfre', 'hang', 'hanker', 'hanki', 'happen', 'happi', 'happier', 'hard', 'harri', 'hasn', 'hate', 'hatr', 'haunt', 'have', 'haven', 'havilland', 'hawaiian', 'hay', 'hayworth', 'hbo', 'he', 'head', 'head-over-heel', 'headband', 'headphon', 'headset', 'healthi', 'hear', 'heard', 'heart', 'heartwarm', 'heat', 'heaven', 'heavi', 'hech', 'heimer', 'heist', 'held', 'helen', 'hell', 'hella', 'hello', 'helm', 'help', 'henc', 'hendrikson', 'her', 'here', 'herea', 'hernandez', 'hero', 'heroin', 'hi', 'hide', 'high', 'high-qual', 'higher', 'highest', 'highi', 'highli', 'highlight', 'hilari', 'hill', 'him', 'himself', 'hing', 'hip', 'histori', 'hit', 'hitch', 'hitchcock', 'hold', 'holder', 'hole', 'holiday', 'holland', 'hollow', 'holster', 'home', 'homemad', 'homework', 'honest', 'honestli', 'honor', 'hook', 'hoot', 'hope', 'hopeless', 'horrend', 'horribl', 'horrifi', 'horror', 'hors', 'host', 'hostess', 'hot', 'hour', 'hoursth', 'hous', 'how', 'howdi', 'howev', 'however-th', 'hs', 'huevo', 'huge', 'hugo', 'hum', 'human', 'humili', 'hummh', 'hummu', 'humor', 'humour', 'hundr', 'hurri', 'hurt', 'husband', 'huston', 'hut', 'hybrid', 'hype', 'hypocrisi', 'i', 'iam', 'ian', 'ice', 'idea', 'ideal', 'identifi', 'idiot', 'idiot-sav', 'idyl', 'if', 'iffi', 'ignor', 'igo', 'ill', 'im', 'imac', 'imag', 'imagin', 'imdb', 'imit', 'immedi', 'impact', 'impecc', 'imperi', 'implaus', 'import', 'imposs', 'impress', 'improp', 'improv', 'improvis', 'impuls', 'in', 'in-hous', 'inappropri', 'incendiari', 'includ', 'incom', 'incomprehens', 'inconsider', 'inconspicu', 'incred', 'inde', 'indescrib', 'indian', 'indic', 'indict', 'individu', 'indoor', 'indulg', 'industri', 'ineptli', 'inexcus', 'inexpens', 'inexperi', 'inexplic', 'infatu', 'inflat', 'inform', 'infra', 'infuri', 'ingredi', 'innoc', 'insan', 'insert', 'insid', 'insincer', 'insipid', 'insomniac', 'inspir', 'instal', 'instanc', 'instant', 'instantli', 'instead', 'instruct', 'instrument', 'insulin', 'insult', 'intang', 'integr', 'intellig', 'intend', 'intens', 'intent', 'interact', 'interest', 'interfac', 'interim', 'intermitt', 'internet', 'interplay', 'interpret', 'interview', 'into', 'inton', 'invent', 'invest', 'invit', 'involv', 'iphon', 'ipod', 'iq', 'ir', 'irda', 'ireland', 'iriv', 'iron', 'ironman', 'ironsid', 'irrit', 'is', 'ishioka', 'isn', 'issu', 'it', 'italian', 'item', 'itself', 'jabra', 'jack', 'jaclyn', 'jalapeno', 'jame', 'jami', 'japanes', 'jason', 'jawbon', 'jean', 'jeff', 'jennif', 'jerk', 'jerki', 'jerri', 'jessic', 'jewel', 'jiggl', 'jim', 'jimmi', 'job', 'joe', 'john', 'join', 'joint', 'joke', 'jonah', 'jone', 'journey', 'joy', 'joyc', 'juano', 'judg', 'judo', 'juic', 'julian', 'junk', 'juri', 'just', 'justic', 'jutland', 'jx-', 'k', 'kabuki', 'kanali', 'kathi', 'keep', 'keira', 'keith', 'kept', 'kevin', 'key', 'keyboard', 'keypad', 'khao', 'kid', 'kidnap', 'kieslowski', 'kill', 'killer', 'kind', 'kinda', 'kindl', 'kit', 'kitchi', 'knew', 'knightley', 'knock', 'know', 'known', 'kotea', 'kri', 'kristoffersen', 'krussel', 'kudo', 'l', 'la', 'lack', 'ladi', 'lame', 'lanc', 'land-lin', 'landscap', 'lang', 'lap', 'laptop', 'larg', 'lassi', 'last', 'latch', 'late', 'later', 'latest', 'latifa', 'latin', 'latter-day', 'laugh', 'laughabl', 'law', 'lawyer', 'layer', 'lazi', 'lb', 'lead', 'leaf', 'leak', 'leap', 'learn', 'least', 'leather', 'leav', 'left', 'leftov', 'leg', 'legal', 'legit', 'lemon', 'length', 'lens', 'leopard', 'less', 'lesser', 'lesser-known', 'lesson', 'lestat', 'let', 'lettuc', 'level', 'lewi', 'lg', 'lie', 'lieuten', 'life', 'lifetim', 'light', 'lighter', 'lightli', 'lightweight', 'like', 'lil', 'lilt', 'limit', 'linda', 'line', 'link', 'linksi', 'lino', 'lion', 'list', 'listen', 'lit', 'liter', 'litter', 'littl', 'live', 'll', 'lo', 'load', 'locat', 'lock', 'loewenhielm', 'logic', 'logitech', 'loneli', 'long', 'long-wear', 'longer', 'look', 'loop', 'loos', 'lord', 'lordi', 'lose', 'lost', 'lot', 'loud', 'loudest', 'loudli', 'loudspeak', 'lousi', 'lovabl', 'love', 'lover', 'low', 'low-budget', 'low-key', 'loyal', 'loyalti', 'luci', 'lucio', 'luck', 'lugosi', 'luke', 'lukewarm', 'lunch', 'luv', 'lyric', 'm', 'mac', 'macbeth', 'machin', 'mad', 'made', 'madhous', 'madison', 'magic', 'mail', 'main', 'mainli', 'maintain', 'major', 'make', 'maker', 'male', 'male-bond', 'mall', 'malta', 'man', 'manag', 'mango', 'mani', 'mansonit', 'manual', 'marbl', 'margarita', 'mari', 'marion', 'mark', 'market', 'marriag', 'marrow', 'martin', 'martini', 'masculin', 'massiv', 'master', 'masterpiec', 'match', 'materi', 'matrix', 'matthew', 'matur', 'may', 'mayb', 'mayo', 'mchatti', 'mclaglen', 'me', 'meal', 'mean', 'meander', 'meant', 'meat', 'meatbal', 'meatloaf', 'mechan', 'media', 'medic', 'mediocr', 'medium', 'meet', 'mega', 'megapixel', 'meh', 'mein', 'meld', 'mellow', 'melodrama', 'melt', 'melvil', 'member', 'memor', 'memori', 'menac', 'mention', 'menu', 'merci', 'meredith', 'merit', 'mesquit', 'mess', 'messag', 'metal', 'metro', 'mexican', 'mgm', 'mic', 'michael', 'mickey', 'microphon', 'microsoft', 'mid', 'middl', 'middle-ag', 'might', 'mighti', 'mile', 'militari', 'min', 'mind', 'mind-bendingli', 'mindblow', 'miner', 'mini-seri', 'mini-usb', 'minor', 'minut', 'mirag', 'miser', 'mishima', 'mislead', 'miss', 'mistak', 'mix', 'miyazaki', 'mmmm', 'mobil', 'mode', 'model', 'modern', 'modest', 'moist', 'mollusk', 'mom', 'moment', 'momentum', 'money', 'monica', 'monkey', 'monolog', 'monoton', 'monster', 'monstrou', 'month', 'monument', 'mood', 'moral', 'more', 'morgan', 'morn', 'moron', 'mortifi', 'most', 'mostli', 'mother', 'motiv', 'moto', 'motor', 'motorola', 'mous', 'mouth', 'move', 'movement', 'movi', 'movie--it', 'movie-go', 'movie-mak', 'mp', 'ms', 'mst', 'much', 'muddi', 'muddl', 'muffin', 'muffl', 'multi-grain', 'multipl', 'muppet', 'murder', 'murki', 'mushroom', 'music', 'must', 'must-hav', 'must-stop', 'my', 'myself', 'mystifi', 'naan', 'nacho', 'nake', 'name', 'nano', 'nargil', 'narr', 'narrat', 'nasti', 'nation', 'natur', 'naughti', 'navig', 'nay', 'nc-', 'nearli', 'neat', 'need', 'needless', 'needlessli', 'neg', 'neglig', 'negulesco', 'neighborhood', 'neither', 'nervou', 'netflix', 'network', 'never', 'nevertheless', 'nevski', 'new', 'next', 'ngage', 'nice', 'nicer', 'nicola', 'night', 'nigiri', 'nine', 'no', 'nobl', 'nobodi', 'noca', 'noir', 'noir-crime-drama', 'nois', 'nokia', 'non-clich', 'non-custom', 'non-exist', 'non-fanc', 'non-linear', 'non-sequel', 'none', 'nonetheless', 'nonsens', 'noodl', 'nor', 'normal', 'north', 'northern', 'nostalgia', 'not', 'notabl', 'notch', 'note', 'noteworthi', 'noth', 'notic', 'novella', 'now', 'nude', 'number', 'numer', 'nun', 'nut', 'nut-bag', 'nyc', 'o', 'oblig', 'obviou', 'obvious', 'occas', 'occasion', 'occupi', 'occur', 'odd', 'of', 'off', 'off----', 'offend', 'offens', 'offer', 'offici', 'often', 'oh', 'ohhh', 'oil', 'ok', 'okay', 'old', 'old-fashion', 'older', 'ole', 'oliv', 'olivia', 'omelet', 'omg', 'omit', 'on', 'on-screen', 'onc', 'one', 'one-dimension', 'onion', 'onli', 'onlin', 'ooz', 'open', 'oper', 'opera', 'opinion', 'opportun', 'oppos', 'option', 'or', 'order', 'organiz', 'origin', 'os', 'oscar', 'other', 'otherwis', 'otto', 'ought', 'our', 'out', 'outdoor', 'outgo', 'outlandish', 'outlet', 'outperform', 'outshin', 'outsid', 'outstand', 'outta', 'oven', 'over', 'over-hip', 'over-pr', 'over-whelm', 'overact', 'overal', 'overcom', 'overcook', 'overdu', 'overhaul', 'overli', 'overnight', 'overnit', 'overpr', 'overrid', 'overt', 'overwhelm', 'overwrought', 'owe', 'owl', 'own', 'owner', 'oy-vey', 'oyster', 'pace', 'pack', 'packag', 'pad', 'paid', 'pain', 'paint', 'pair', 'palanc', 'palat', 'pale', 'palm', 'palmtop', 'pan', 'pancak', 'panna', 'pant', 'paolo', 'pap', 'paper', 'par', 'parent', 'park', 'part', 'partak', 'parti', 'particular', 'particularli', 'pass', 'passion', 'past', 'pasta', 'pastri', 'pat', 'patent', 'pathet', 'patient', 'patio', 'patriot', 'patron', 'patti', 'paul', 'pay', 'pc', 'pda', 'pea', 'peach', 'peachy-keen', 'peak', 'peanut', 'pear', 'pecan', 'peculiar', 'pedest', 'pen', 'pencil', 'penn', 'penni', 'peopl', 'pepper', 'perfect', 'perfectli', 'perform', 'perhap', 'period', 'perpar', 'perplex', 'person', 'peter', 'petrifi', 'petroleum', 'petti', 'pg', 'pg-', 'pg-rate', 'phantasm', 'philadelphia', 'philippa', 'pho', 'phoenix', 'phone', 'photo', 'photograph', 'photographi', 'phrase', 'pictur', 'piec', 'pile', 'pillow', 'pine', 'pineappl', 'pink', 'pissd', 'pita', 'piti', 'pixar', 'pixel', 'pizza', 'place', 'plain', 'plan', 'plane', 'plant', 'plantain', 'plantron', 'plastic', 'plate', 'plater', 'platter', 'play', 'player', 'pleas', 'pleasant', 'pleaser', 'pleather', 'plenti', 'plethora', 'plot', 'plu', 'plug', 'pm', 'pneumat', 'pocket', 'poetri', 'poignant', 'point', 'pointillist', 'pointless', 'pois', 'poler', 'polit', 'poop', 'poor', 'poorli', 'pop', 'popular', 'pork', 'port', 'portabl', 'portion', 'portrait', 'portray', 'posit', 'possibl', 'post', 'postino', 'pot', 'potato', 'potenti', 'pour', 'powder', 'power', 'powerhous', 'practic', 'pray', 'precis', 'predict', 'prefer', 'prejudic', 'prelud', 'premis', 'premium', 'prepar', 'presenc', 'present', 'presid', 'pretenti', 'pretext', 'pretti', 'prettier', 'prevent', 'previou', 'price', 'pricey', 'primal', 'primari', 'prime', 'print', 'privileg', 'pro', 'probabl', 'problem', 'procedur', 'proceed', 'process', 'proclaim', 'produc', 'product', 'profession', 'professor', 'profiterol', 'profound', 'program', 'promis', 'promot', 'prompt', 'promptli', 'prone', 'propaganda', 'properli', 'protect', 'protector', 'proud', 'proudli', 'proven', 'provid', 'provok', 'ps', 'pseudo-satan', 'psych', 'psycholog', 'psychot', 'pub', 'public', 'publicli', 'puck', 'puff', 'pull', 'pumpkin', 'punch', 'punish', 'puppet', 'purchas', 'purchase-', 'pure', 'push', 'put', 'puzzle-solv', 'pyromaniac', 'q', 'qu', 'quaid', 'quaint', 'qualiti', 'quantiti', 'question', 'quick', 'quicker', 'quickli', 'quiet', 'quinn', 'quit', 'qwerti', 'r', 'race', 'racial', 'racism', 'radiant', 'rage', 'ramsey', 'ranch', 'ranchero', 'random', 'randomli', 'rang', 'rank', 'rapidli', 'rare', 'raspberri', 'rate', 'rather', 'ratio', 'rave', 'raver', 'ravoli', 'raw', 'ray', 'razor', 'razr', 're', 're-set', 'reach', 'reaction', 'read', 'reader', 'readi', 'real', 'real-world', 'realist', 'realiti', 'realiz', 'realli', 'reason', 'recal', 'reccomend', 'receipt', 'receiv', 'recent', 'recept', 'recess', 'recharg', 'reciev', 'recognit', 'recommend', 'reconcili', 'recov', 'red', 'redeem', 'reenact', 'refer', 'refil', 'refresh', 'refri', 'refund', 'refurb', 'refus', 'regard', 'regardless', 'regist', 'regret', 'regrett', 'regular', 'regularli', 'reject', 'rel', 'relat', 'relationship', 'relax', 'releas', 'reli', 'relief', 'relleno', 'reloc', 'remain', 'remak', 'remark', 'rememb', 'remind', 'remors', 'remot', 'remov', 'render', 'rendit', 'renown', 'rent', 'reoccur', 'repair', 'repeat', 'repertori', 'replac', 'replacementr', 'replenish', 'report', 'repres', 'request', 'requir', 'research', 'reserv', 'resolut', 'resound', 'respect', 'rest', 'restart', 'restaur', 'restor', 'restrain', 'result', 'resum', 'return', 'reveal', 'revers', 'reverse-stereotyp', 'review', 'revisit', 'rge', 'ri', 'rib', 'ribey', 'rice', 'rick', 'ride', 'ridicul', 'right', 'riington', 'ring', 'rington', 'rins', 'riot', 'rip', 'rise', 'risk', 'rita', 'rivet', 'road', 'roam', 'roast', 'robert', 'robot', 'rochon--wa', 'rock', 'rocket', 'roeg', 'role', 'roll', 'roller', 'room', 'roosevelt', 'rough', 'round', 'routin', 'row', 'rowdi', 'rpg', 'rpger', 'rubber', 'rubin', 'rude', 'run', 'rush', 'ruthless', 'ryan', 's', 'sack', 'sad', 'sadli', 'saffron', 'saggi', 'said', 'sake', 'sal', 'salad', 'salesman', 'salmon', 'salsa', 'salt', 'salti', 'same', 'sampl', 'samsung', 'san', 'sand', 'sandra', 'sandwich', 'sangria', 'sanyo', 'sappiest', 'sarcophag', 'sashimi', 'sat', 'satifi', 'satisfi', 'satisif', 'sauc', 'saus', 'savala', 'save', 'savor', 'saw', 'say', 'scale', 'scallop', 'scare', 'scari', 'scene', 'sceneri', 'sch-r', 'schill', 'schizophren', 'school', 'schooler', 'schrader', 'schultz', 'sci-fi', 'scienc', 'scientist', 'score', 'scot', 'scottsdal', 'scratch', 'scream', 'screami', 'screen', 'screenwrit', 'scrimm', 'script', 'sculptur', 'sea', 'seafood', 'seal', 'seamless', 'seamlessli', 'sean', 'search', 'season', 'seat', 'second', 'secondari', 'secondli', 'section', 'secur', 'securli', 'see', 'seem', 'seen', 'select', 'self', 'self-discoveri', 'self-indulg', 'self-preserv', 'self-respect', 'sell', 'seller', 'send', 'senior', 'sens', 'sensibl', 'sensit', 'sensor', 'sent', 'sentiment', 'sequel', 'sequenc', 'sergeant', 'seri', 'seriou', 'serious', 'serivc', 'serv', 'server', 'servic', 'set', 'setup', 'seuss', 'sever', 'sex', 'sex-obsess', 'shakespear', 'shall', 'shallow', 'shame', 'shape', 'share', 'sharp', 'sharpli', 'shatter', 'shawarrrrrrma', 'she', 'shed', 'sheer', 'shelf', 'shell', 'shelv', 'shenanigan', 'shine', 'ship', 'shipment', 'shirley', 'shirt', 'shock', 'shoe', 'shoot', 'shooter', 'shop', 'short', 'shortlist', 'shot', 'should', 'shouldn', 'shouldv', 'shout', 'show', 'showcas', 'shower', 'shrimp', 'shrimp-', 'shut-down', 'sibl', 'sick', 'side', 'sidelin', 'sight', 'sign', 'signal', 'signific', 'significantli', 'silent', 'silli', 'sim', 'similar', 'similarli', 'simpl', 'simpler', 'simpli', 'simplifi', 'sin', 'sinc', 'sing', 'singl', 'sink', 'sister', 'sit', 'sit-down', 'site', 'situat', 'six', 'size', 'skill', 'skimp', 'skip', 'skype', 'slacker', 'slaw', 'sleek', 'sleep', 'slice', 'slid', 'slide', 'slider', 'slideshow', 'slightest', 'slightli', 'slim', 'slimi', 'slip', 'sloppi', 'slow', 'slow-mot', 'slow-mov', 'slowli', 'slur', 'smack', 'small', 'smaller', 'smallest', 'smart', 'smear', 'smell', 'smile', 'smith', 'smoke', 'smooth', 'smoother', 'smoothi', 'smoothli', 'smudg', 'snap', 'snider', 'snow', 'snug', 'so', 'soap', 'soft', 'softwar', 'soggi', 'soi', 'sold', 'sole', 'solid', 'solidifi', 'some', 'somehow', 'someon', 'someth', 'sometim', 'somewhat', 'somewher', 'son', 'song', 'soni', 'soon', 'sooner', 'soooo', 'sooooo', 'sore', 'sorrentino', 'sorri', 'sort', 'soul', 'sound', 'sound-wis', 'soundtrack', 'soup', 'sour', 'sourc', 'south', 'southern', 'southwest', 'soyo', 'space', 'spacek', 'spacey', 'spaghetti', 'span', 'speak', 'speaker', 'speakerphon', 'special', 'speed', 'spend', 'spent', 'spew', 'sphere', 'spi', 'spice', 'spici', 'spiffi', 'spinach', 'spinn', 'splendid', 'spoil', 'spoiler', 'sport', 'spot', 'spotti', 'spring', 'sprint', 'squib', 'stabl', 'staff', 'stage', 'stagey', 'stagi', 'stale', 'stand', 'standard', 'stanwyck', 'star', 'starlet', 'start', 'startac', 'starter', 'starv', 'state', 'state-of-the-art', 'static', 'station', 'stay', 'steak', 'steakhous', 'steal', 'steamboat', 'steel', 'steep', 'steer', 'steiner', 'step', 'stephen', 'stereotyp', 'steve', 'stewart', 'stick', 'still', 'stink', 'stinker', 'stir', 'stock', 'stoic', 'stomach', 'stood', 'stop', 'storag', 'store', 'stori', 'storm', 'storylin', 'strang', 'stranger', 'stratu', 'straw', 'strawberri', 'stream', 'street', 'strength', 'stress', 'stretch', 'strident', 'strike', 'string', 'strip', 'strive', 'strong', 'struck', 'structur', 'struggl', 'stuart', 'student', 'studi', 'stuf', 'stuff', 'stun', 'stupid', 'sturdi', 'style', 'stylish', 'styliz', 'styrofoam', 'sub', 'sub-genr', 'sub-par', 'sub-plot', 'sublim', 'submerg', 'subtl', 'subway', 'succeed', 'succul', 'such', 'suck', 'sucker', 'sudden', 'suffer', 'sugar', 'sugari', 'suggest', 'suit', 'sum', 'summar', 'summari', 'summer', 'sun', 'sunday', 'sunglass', 'super', 'super-intellig', 'superb', 'superbad', 'superbl', 'superfast', 'superfici', 'supernatur', 'supertooth', 'support', 'supposedli', 'sure', 'surefir', 'surf', 'surfac', 'surpris', 'surprisingli', 'surround', 'surviv', 'survivor', 'sushi', 'suspens', 'sven', 'sweep', 'sweet', 'switch', 'swivel', 'swung', 'sympathet', 'sync', 'syrupi', 'system', 't', 't-mobil', 'tabl', 'taco', 'tailor', 'take', 'taken', 'takeout', 'tale', 'talent', 'talk', 'tank', 'tap', 'tapa', 'tape', 'tardi', 'tartar', 'task', 'tast', 'tasteless', 'tasti', 'tater', 'taxidermist', 'taylor', 'tea', 'teach', 'teacher', 'team', 'tear', 'tech', 'technolog', 'teddi', 'tedium', 'teen', 'teeth', 'telephon', 'televis', 'tell', 'telli', 'temp', 'tempera', 'ten', 'tender', 'tension', 'tepid', 'term', 'terminolog', 'terribl', 'terrif', 'terror', 'text', 'textur', 'th', 'thai', 'than', 'thank', 'that', 'that-suck', 'the', 'theater', 'theatr', 'theatric', 'theft', 'their', 'them', 'theme', 'theme--at', 'themselv', 'then', 'theori', 'there', 'thereplac', 'these', 'they', 'thi', 'thick', 'thin', 'thing', 'think', 'thinli', 'third', 'thirti', 'thomerson', 'thorn', 'thoroughli', 'thorsen', 'those', 'though', 'thought', 'thought-provok', 'thousand', 'three', 'three-pack', 'threshold', 'threw', 'thrill', 'thriller', 'through', 'throughout', 'throwback', 'thrown', 'thru', 'thu', 'thug', 'thumb', 'thumper', 'thunderbird', 'tick', 'ticket', 'tie', 'tigerlilli', 'tight', 'tightli', 'time', 'time-wast', 'timefram', 'timeless', 'timer', 'tini', 'tinni', 'tip', 'tiramisu', 'tire', 'titl', 'titta', 'to', 'to-go', 'toactiv', 'toast', 'today', 'togeth', 'toilet', 'told', 'toler', 'tom', 'tomato', 'tomorrow', 'ton', 'tone', 'tongu', 'toni', 'tonight', 'too', 'took', 'tool', 'toon', 'tooth', 'top', 'topic', 'toro', 'tortur', 'tot', 'total', 'touch', 'tough', 'toward', 'tower', 'town', 'townsend', 'tracfon', 'tracfonewebsit', 'track', 'tradit', 'trailer', 'train', 'transceiv', 'transcend', 'transfer', 'transform', 'translat', 'transmiss', 'transmit', 'transmitt', 'trap', 'trash', 'travl', 'treacheri', 'treasur', 'treat', 'trek', 'tremend', 'treo', 'tri', 'tribut', 'tricki', 'trilog', 'trim', 'trip', 'trippi', 'triumph', 'trond', 'trooper', 'troubl', 'true', 'truffl', 'truli', 'trumbul', 'trumpet', 'trunk', 'trust', 'truth', 'tryst', 'tsunami', 'tucson', 'tummi', 'tuna', 'tune', 'turkey', 'turn', 'tv', 'twice', 'twirl', 'twist', 'two', 'type', 'typic', 'u', 'ugli', 'ugliest', 'uhura', 'ultra-cheap', 'um', 'unaccept', 'unbear', 'unbeliev', 'uncal', 'uncomfort', 'uncondit', 'unconvinc', 'under', 'under-servic', 'underact', 'underappreci', 'underbit', 'undercook', 'underli', 'underlin', 'underneath', 'underr', 'underst', 'understand', 'understat', 'underton', 'underwat', 'undoubtedli', 'unemploy', 'uneth', 'unfold', 'unforgett', 'unfortun', 'unfunni', 'unhappi', 'uninspir', 'unintellig', 'unintent', 'uninterest', 'union', 'uniqu', 'unit', 'univers', 'unknown', 'unless', 'unlik', 'unlock', 'unmitig', 'unmov', 'unnecessari', 'unneed', 'unorigin', 'unpleas', 'unpredict', 'unprofession', 'unreal', 'unrealist', 'unrecogniz', 'unrecommend', 'unreli', 'unremark', 'unrestrain', 'unsatisfactori', 'unsatisfi', 'until', 'untoast', 'unus', 'unwatch', 'unwelcom', 'unwrap', 'up', 'up-and-com', 'upa', 'updat', 'upgrad', 'uplift', 'upload', 'upper', 'upstair', 'uptight', 'ursula', 'us', 'usabl', 'usag', 'usb', 'use', 'useless', 'user', 'ussr', 'usual', 'utter', 'utterli', 'v', 'vacant', 'vain', 'valentin', 'valu', 'vampir', 'vandiv', 'vanilla', 'variat', 've', 'veal', 'vega', 'vegan', 'veget', 'veggi', 'veggitarian', 'vehicl', 'velvet', 'ventur', 'ventura', 'venu', 'verbal', 'verbatim', 'verg', 'veri', 'verizon', 'versatil', 'version', 'vessel', 'veteran', 'via', 'vibe', 'victor', 'video', 'view', 'viewer', 'villain', 'vinaigrett', 'vinegrett', 'violin-play', 'violinist', 'virgin', 'viru', 'vision', 'visit', 'visual', 'vital', 'vivian', 'vivid', 'vodka', 'voic', 'voice-ov', 'volatil', 'volcano', 'voltag', 'volum', 'vomit', 'voodoo', 'voyag', 'vulcan', 'vx', 'w', 'wa', 'waaaaaayyyyyyyyyi', 'waaay', 'wagyu', 'wait', 'waiter', 'waitress', 'walk', 'walkman', 'wall', 'wallet', 'want', 'war', 'warm', 'warmer', 'warmth', 'warn', 'warranti', 'wash', 'wasn', 'wast', 'watch', 'watchabl', 'water', 'waterproof', 'watkin', 'watson', 'wave', 'way', 'waylaid', 'wayn', 'wayyy', 'wb', 'we', 'weak', 'weaker', 'wear', 'weav', 'web', 'websit', 'wed', 'week', 'weekend', 'weekli', 'weight', 'weird', 'welcom', 'well', 'well--it', 'well-design', 'well-don', 'well-pac', 'went', 'were', 'weren', 'what', 'whatev', 'whatsoev', 'when', 'whenev', 'where', 'whether', 'whi', 'which', 'while', 'whine', 'whini', 'whistl', 'white', 'who', 'whoa', 'whoever', 'whole', 'wholesom', 'whom', 'whose', 'wi-fi', 'widmark', 'wienerschnitzel', 'wife', 'wife-to-b', 'wih', 'wild', 'wildli', 'wili', 'wilkinson', 'will', 'willi', 'win', 'wind', 'wind-resist', 'window', 'wine', 'wing', 'winner', 'wipe', 'wire', 'wirefli', 'wireless', 'wise', 'wish', 'wit', 'with', 'within', 'without', 'witti', 'wittic', 'woa', 'wobbl', 'women', 'won', 'wonder', 'wont', 'wonton', 'woo', 'wood', 'wooden', 'word', 'word-of-mouth', 'work', 'worker', 'world', 'world-weari', 'worn-out', 'worri', 'wors', 'worst', 'worth', 'worthi', 'worthless', 'worthwhil', 'would', 'would-b', 'wouldn', 'wouldnt', 'wound', 'woven', 'wow', 'wrap', 'write', 'writer', 'written', 'wrong', 'wrongli', 'wrote', 'x', 'ya', 'yama', 'yawn', 'ye', 'yeah', 'year', 'yell', 'yellow', 'yellowtail', 'yelper', 'yet', 'you', 'you--do', 'young', 'younger', 'your', 'yourself', 'youth', 'youtub', 'yucki', 'yukon', 'yum', 'yummi', 'yun', 'z', 'zero', 'zillion', 'zombi']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beginning-leone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>-drink</th>\n",
       "      <th>-good</th>\n",
       "      <th>-mi</th>\n",
       "      <th>-year</th>\n",
       "      <th>a</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abil</th>\n",
       "      <th>...</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yucki</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummi</th>\n",
       "      <th>yun</th>\n",
       "      <th>z</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows  3620 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      -  --  -drink  -good  -mi  -year  a  abandon  abhor  abil  ...  youtub  \\\n",
       "0     0   0       0      0    0      0  0        0      0     0  ...       0   \n",
       "1     0   0       0      0    0      0  0        0      0     0  ...       0   \n",
       "2     0   0       0      0    0      0  0        0      0     0  ...       0   \n",
       "3     0   0       0      0    0      0  0        0      0     0  ...       0   \n",
       "4     0   0       0      0    0      0  0        0      0     0  ...       0   \n",
       "...  ..  ..     ...    ...  ...    ... ..      ...    ...   ...  ...     ...   \n",
       "2395  0   0       0      0    0      0  0        0      0     0  ...       0   \n",
       "2396  0   0       0      0    0      0  0        0      0     0  ...       0   \n",
       "2397  0   0       0      0    0      0  0        0      0     0  ...       0   \n",
       "2398  0   0       0      0    0      0  1        0      0     0  ...       0   \n",
       "2399  0   0       0      0    0      0  0        0      0     0  ...       0   \n",
       "\n",
       "      yucki  yukon  yum  yummi  yun  z  zero  zillion  zombi  \n",
       "0         0      0    0      0    0  0     0        0      0  \n",
       "1         0      0    0      0    0  0     0        0      0  \n",
       "2         0      0    0      0    0  0     0        0      0  \n",
       "3         0      0    0      0    0  0     0        0      0  \n",
       "4         0      0    0      0    0  0     0        0      0  \n",
       "...     ...    ...  ...    ...  ... ..   ...      ...    ...  \n",
       "2395      0      0    0      0    0  0     0        0      0  \n",
       "2396      0      0    0      0    0  0     0        0      0  \n",
       "2397      0      0    0      0    0  0     0        0      0  \n",
       "2398      0      0    0      0    0  0     0        0      0  \n",
       "2399      0      0    0      0    0  0     0        0      0  \n",
       "\n",
       "[2400 rows x 3620 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x.toarray(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-precipitation",
   "metadata": {},
   "source": [
    "## TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adverse-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "complicated-grant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3620"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer = TfidfVectorizer(tokenizer=stemming_tokenizer, use_idf = True)\n",
    "x = tf_vectorizer.fit_transform(x_train)\n",
    "x_te = tf_vectorizer.transform(x_test)\n",
    "features = tf_vectorizer.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-marriage",
   "metadata": {},
   "source": [
    "# Neuronetwork #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "lined-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from MLPClassifierWithSolverLBFGS import MLPClassifierLBFGS\n",
    "\n",
    "# from viz_tools_for_binary_classifier import plot_pretty_probabilities_for_clf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "robust-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.toarray()\n",
    "y = y_train_df['is_positive_sentiment'].to_numpy()\n",
    "feat_num = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "written-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-training",
   "metadata": {},
   "source": [
    "### CV without best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "comic-hollow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished SGD run  1/16 after   30.0 sec | 400 epochs | NOT converged | loss 0.023\n",
      "finished SGD run  2/16 after   30.9 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run  3/16 after   32.0 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run  4/16 after   32.5 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run  5/16 after   30.5 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run  6/16 after   32.1 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run  7/16 after   31.3 sec | 400 epochs | NOT converged | loss 0.021\n",
      "finished SGD run  8/16 after   31.6 sec | 400 epochs | NOT converged | loss 0.023\n",
      "finished SGD run  9/16 after   30.8 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run 10/16 after   30.7 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run 11/16 after   31.0 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run 12/16 after   32.4 sec | 400 epochs | NOT converged | loss 0.023\n",
      "finished SGD run 13/16 after   31.2 sec | 400 epochs | NOT converged | loss 0.020\n",
      "finished SGD run 14/16 after   31.9 sec | 400 epochs | NOT converged | loss 0.022\n",
      "finished SGD run 15/16 after   34.5 sec | 400 epochs | NOT converged | loss 0.020\n",
      "finished SGD run 16/16 after   29.2 sec | 400 epochs | NOT converged | loss 0.022\n"
     ]
    }
   ],
   "source": [
    "n_runs = 16\n",
    "tr_classifierLOG_SGD = list()\n",
    "\n",
    "for i in range(n_runs):\n",
    "    start_time_sec = time.time()\n",
    "    mlp_sgd = MLPClassifier(\n",
    "        hidden_layer_sizes=[2],\n",
    "        activation='logistic',\n",
    "        alpha=0.0001,\n",
    "        max_iter=400, tol=1e-8,\n",
    "        random_state=i,\n",
    "        solver='sgd', batch_size=10,\n",
    "        learning_rate='adaptive', learning_rate_init=0.1, momentum=0.0,\n",
    "        )\n",
    "    with warnings.catch_warnings(record=True) as warn_list:\n",
    "        clf = mlp_sgd.fit(X, y)\n",
    "    mlp_sgd.did_converge = True if len(warn_list) == 0 else False\n",
    "    elapsed_time_sec = time.time() - start_time_sec\n",
    "    print('finished SGD run %2d/%d after %6.1f sec | %3d epochs | %s | loss %.3f' % (\n",
    "        i+1, n_runs, elapsed_time_sec,\n",
    "        len(mlp_sgd.loss_curve_),\n",
    "            'converged    ' if mlp_sgd.did_converge else 'NOT converged',\n",
    "            mlp_sgd.loss_))\n",
    "\n",
    "    tr_classifierLOG_SGD.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------\\nClassify with base data, 5 folds\\n-----------------\")\n",
    "\n",
    "k = 3\n",
    "kfold = KFold(n_splits=k)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "# model = LogisticRegression()\n",
    "\n",
    "for train_idx, test_idx in kfold.split(X):\n",
    "    X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    \n",
    "    mlp_sgd = MLPClassifier(\n",
    "            hidden_layer_sizes=[2],\n",
    "            activation='logistic',\n",
    "            alpha=0.0001,\n",
    "            max_iter=400, tol=1e-8,\n",
    "            random_state=i,\n",
    "            solver='sgd', batch_size=10,\n",
    "            learning_rate='adaptive', learning_rate_init=0.1, momentum=0.0,\n",
    "            )\n",
    "        with warnings.catch_warnings(record=True) as warn_list:\n",
    "            clf = mlp_sgd.fit(X_train, y_train)\n",
    "            pred_train = clf.predict_proba(X_train)\n",
    "            pred_test = clf.predict_proba(X_test)\n",
    "            \n",
    "            score_train = clf.score(X_train, y_train)\n",
    "            score_test = clf.score(X_test, y_test)\n",
    "        mlp_sgd.did_converge = True if len(warn_list) == 0 else False\n",
    "        elapsed_time_sec = time.time() - start_time_sec\n",
    "        print('finished SGD run after %6.1f sec | %3d epochs | %s | loss %.3f' % (\n",
    "              elapsed_time_sec,\n",
    "            len(mlp_sgd.loss_curve_),\n",
    "                'converged    ' if mlp_sgd.did_converge else 'NOT converged',\n",
    "                mlp_sgd.loss_))\n",
    "\n",
    "        tr_classifierLOG_SGD.append(clf)\n",
    "    \n",
    "\n",
    " \n",
    "    \n",
    "    train_scores.append(score_train)\n",
    "    test_scores.append(score_test)\n",
    "    \n",
    "print(\"\\nAverage train accuracy: \", np.average(score_train))\n",
    "print(\"Average test accuracy: \", np.average(score_test))\n",
    "\n",
    "\n",
    "    n_runs = 16\n",
    "    tr_classifierLOG_SGD = list()\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        start_time_sec = time.time()\n",
    "        mlp_sgd = MLPClassifier(\n",
    "            hidden_layer_sizes=[2],\n",
    "            activation='logistic',\n",
    "            alpha=0.0001,\n",
    "            max_iter=400, tol=1e-8,\n",
    "            random_state=i,\n",
    "            solver='sgd', batch_size=10,\n",
    "            learning_rate='adaptive', learning_rate_init=0.1, momentum=0.0,\n",
    "            )\n",
    "        with warnings.catch_warnings(record=True) as warn_list:\n",
    "            clf = mlp_sgd.fit(X, y)\n",
    "        mlp_sgd.did_converge = True if len(warn_list) == 0 else False\n",
    "        elapsed_time_sec = time.time() - start_time_sec\n",
    "        print('finished SGD run %2d/%d after %6.1f sec | %3d epochs | %s | loss %.3f' % (\n",
    "            i+1, n_runs, elapsed_time_sec,\n",
    "            len(mlp_sgd.loss_curve_),\n",
    "                'converged    ' if mlp_sgd.did_converge else 'NOT converged',\n",
    "                mlp_sgd.loss_))\n",
    "\n",
    "        tr_classifierLOG_SGD.append(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-sally",
   "metadata": {},
   "source": [
    "### CV with the best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_grid = np.logspace(-9, 6, 31)\n",
    "model_list = []\n",
    "aver_train_score = []\n",
    "aver_test_score = []\n",
    "aver_train_loss = []\n",
    "aver_test_loss = []\n",
    "\n",
    "\n",
    "for C in C_grid:\n",
    "    k = 3\n",
    "    kfold = KFold(n_splits=k)\n",
    "    \n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "\n",
    "    model = sklearn.linear_model.LogisticRegression(C=C,solver='liblinear')\n",
    "    for train_idx, test_idx in kfold.split(X):\n",
    "        X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        pred_train = model.predict_proba(X_train)\n",
    "        pred_test = model.predict_proba(X_test)\n",
    "\n",
    "        # Score\n",
    "        score_train = model.score(X_train, y_train)\n",
    "        score_test = model.score(X_test, y_test)\n",
    "#         print(\"Train score: \", score_train)\n",
    "#         print(\"Test score: \", score_test)\n",
    "        train_scores.append(score_train)\n",
    "        test_scores.append(score_test)\n",
    "        \n",
    "        # Log loss\n",
    "        log_loss_train = sklearn.metrics.log_loss(y_train,pred_train)\n",
    "        log_loss_test = sklearn.metrics.log_loss(y_test,pred_test)\n",
    "#         print(\"Train loss: \", log_loss_train)\n",
    "#         print(\"Test loss: \", log_loss_test)\n",
    "        train_loss.append(log_loss_train)\n",
    "        test_loss.append(log_loss_test)\n",
    "        \n",
    "        \n",
    "    print(\"\\nFor C value : \", C)\n",
    "    print(\"\\nAverage train accuracy: \", np.average(score_train))\n",
    "    print(\"Average test accuracy: \", np.average(score_test))\n",
    "    print(\"\\nAverage train loss: \", np.average(train_loss))\n",
    "    print(\"Average test loss: \", np.average(test_loss))\n",
    "    \n",
    "    print('------------------------------------------------\\n')\n",
    "    \n",
    "    model_list.append(model)\n",
    "    aver_train_score.append(np.average(score_train))\n",
    "    aver_test_score.append(np.average(score_test))\n",
    "    aver_train_loss.append(np.average(train_loss))\n",
    "    aver_test_loss.append(np.average(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the results in clear tabular format\n",
    "pd.DataFrame(np.transpose([aver_train_score, aver_test_score, aver_train_loss, aver_test_loss]), columns=['train accuracy', 'test accuracy', 'train loss', 'test loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-quest",
   "metadata": {},
   "source": [
    "#### Best Log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = min(aver_test_loss)\n",
    "index_N2 = aver_test_loss.index(min_loss)\n",
    "best_C =  C_grid[index_N2]\n",
    "best_model = model_list[index_N2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-monday",
   "metadata": {},
   "source": [
    "#### stability across Kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------\\nClassify with base data, 5 folds\\n-----------------\")\n",
    "\n",
    "K = [2,3,4,5,6,7,8,9,10,11,12]\n",
    "K_train_loss = []\n",
    "K_test_loss = []\n",
    "for k in K:\n",
    "    kfold = KFold(n_splits=k)\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for train_idx, test_idx in kfold.split(X):\n",
    "        shuffler = np.random.permutation(len(X))\n",
    "        X_shuffled = X[shuffler]\n",
    "        y_shuffled = y[shuffler]\n",
    "        X_train, X_test = X_shuffled[train_idx,:], X_shuffled[test_idx,:]\n",
    "        y_train, y_test = y_shuffled[train_idx], y_shuffled[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        pred_train = best_model.predict_proba(X_train)\n",
    "        pred_test = best_model.predict_proba(X_test)\n",
    "\n",
    "        score_train = best_model.score(X_train, y_train)\n",
    "        score_test = best_model.score(X_test, y_test)\n",
    "        train_scores.append(score_train)\n",
    "        test_scores.append(score_test)\n",
    "        \n",
    "        log_loss_train = sklearn.metrics.log_loss(y_train,pred_train)\n",
    "        log_loss_test = sklearn.metrics.log_loss(y_test,pred_test)\n",
    "        \n",
    "        train_loss.append(log_loss_train)\n",
    "        test_loss.append(log_loss_test)\n",
    "\n",
    "    print(\"\\nAverage train accuracy: \", np.average(score_train))\n",
    "    print(\"Average test accuracy: \", np.average(score_test))\n",
    "    print(\"Average train loss: \", np.average(train_loss))\n",
    "    print(\"Average test loss: \", np.average(test_loss))\n",
    "    \n",
    "    K_train_loss.append(np.average(train_loss))\n",
    "    K_test_loss.append(np.average(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('K from 2 to 12');\n",
    "plt.ylabel('logistic loss');\n",
    "\n",
    "sns.lineplot(x = K, y = K_train_loss, label = \"Train Loss\", color = \"red\", marker='o')\n",
    "sns.lineplot(x = K, y = K_test_loss, label = \"Test Loss\", color = \"blue\", marker='o')\n",
    "\n",
    "# show a legend on the plot \n",
    "plt.legend() \n",
    "plt.title('Log loss across K values')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('standard deviation for training set: %.3f  ' %np.std(K_train_loss))\n",
    "print('standard deviation for testing set: %.3f  ' %np.std(K_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('log10(C)');\n",
    "plt.ylabel('logistic loss');\n",
    "plt.ylim([0.0, 1]);\n",
    "\n",
    "sns.lineplot(x = np.log10(C_grid), y = aver_train_loss, label = \"Train Loss\", color = \"red\", marker='o')\n",
    "sns.lineplot(x = np.log10(C_grid), y = aver_test_loss,label = \"Test Loss\", color = \"blue\", marker='o')\n",
    "\n",
    "# show a legend on the plot \n",
    "plt.legend() \n",
    "plt.title('Logistic loss on C-grid')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best C-value for LR: %.3f\" % best_C) \n",
    "print(\"Test set log-loss at best C-value: %.4f\" % min_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-satin",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "yproba1_test = best_model.predict_proba(x_te)[:, 1] \n",
    "np.savetxt('yproba1_test.txt', yproba1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-circuit",
   "metadata": {},
   "source": [
    "# Neuronetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-paste",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 16\n",
    "tr_classifierLBFS = list()\n",
    "\n",
    "for i in range(n_runs):\n",
    "    start_time_sec = time.time()\n",
    "    mlp_lbfgs = MLPClassifier(\n",
    "        hidden_layer_sizes=[2],\n",
    "        activation='relu',\n",
    "        alpha=0.0001,\n",
    "        max_iter=200, tol=1e-6,\n",
    "        random_state=i,\n",
    "        )\n",
    "    with warnings.catch_warnings(record=True) as warn_list:\n",
    "        clf = mlp_lbfgs.fit(x_tr_N2, y_tr_N)\n",
    "    elapsed_time_sec = time.time() - start_time_sec\n",
    "    print('finished LBFGS run %2d/%d after %6.1f sec | %3d iters | %s | loss %.3f' % (\n",
    "        i+1, n_runs, elapsed_time_sec,\n",
    "        len(mlp_lbfgs.loss_curve_),\n",
    "        'converged   ' if mlp_lbfgs.did_converge else 'NOT converged',\n",
    "        mlp_lbfgs.loss_))\n",
    "        \n",
    "    tr_classifierLBFS.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-commodity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
